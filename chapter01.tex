\chapter{Introduction}
\label{chap:intro}

\deleted{The trend of both curiosity and profit-driven human development has caused a surge in the amount of openly accessible raw data. More often than not, the data is generated much faster than it can be processed into something interpretable	or useful. In the endeavor of keeping up with the inflow of information, there are two major factors that significantly hinder our progress. First, Mooreâ€™s law implicitly sets a physical limit to the number of transistors that can be placed on a chip, consequently limiting how powerful and how fast electronic systems can become (barring a paradigm shift in the fundamental design of semiconductors). The second is the Nyquist-Shannon sampling theorem (NST), which limits the range of frequencies a recording device can successfully capture.}

This study explores the use of compressive sensing (CS)\replaced{,}{---} an emergent sampling theorem that allows reconstruction of signals from much fewer samples than required by the Nyquist-Shannon sampling theorem (NST). In this framework, the computational burden of encoding/decoding a signal is shifted from the sampling device to the device performing reconstruction, decompression, or other modes of post-processing. As such, there exist many ways to reconstruct a signal from compressive measurements.

CS has found its applications in simple audio signals containing stable frequencies (such as pure tones \cite{Mathew2016,Andras2018}) and dynamic frequencies (such as speech \cite{Low2013,Low2018,Abrol2015}), images \cite{Mo2013,Zhou2016,Romero2016}, and grayscale videos \cite{Liu2014,Chen2014}. The formulation of a sensing matrix in CS requires a basis conforming to some uniform uncertainty principle, and most common starting points would be partial discrete cosine transform (DCT) matrices or partial discrete wavelet transform (DWT) matrices. \deleted{Recent studies, however, have shown that learned bases perform much better on more complex signals, i.e., those that would be typically encountered in real life situations. The learning algorithms associated with the construction of these bases range from classical iterative methods, which have long been used in optimization problems, to the more contemporary machine learning methods.}


\section{Related literature}
\label{sec:rrl}
In 2004, Cand\`{e}s, Romberg, Tao \cite{Candes2006}, and Donoho \cite{Donoho2006} \added{both asked and answered the questions that birthed} the field which we now know as compressive sensing. The methods in CS apply concepts from time-frequency uncertainty principles \cite{Donoho2001} and sparse representations, which were studied rigorously by Donoho and Elad \cite{Donoho2003}. \deleted{CS can be viewed as a strategic undersampling method: the signal is sampled at random points in the real domain, and the ratio of the indices where it is sampled to the size of the signal can be associated with some quasi-frequency which may be lower than the Nyquist rate.}

\added{Linh-Trung et al.} \cite{LinhTrung2008} demonstrated the use of deterministic chaos filters to acquire samples instead of random distributions. \deleted{Sampling using a Gaussian-Logistic map was applied to acoustic signals in \cite{Mathew2016}.} Normally, a deterministic chaotic function needs one or more \replaced{initial value parameters}{initialization values as a ``seed''}, and the sequence \deleted{of numbers} produced by different combinations of initial values rapidly diverge from each other. This phenomenon led to investigation of the use of CS as an encryption algorithm. Simultaneous compression and encryption was achieved by \cite{Zhou2016}, and it was found that the produced sequences were sensitive to initial value perturbations on the order of $10^{-15}$. Their image compression-encryption model via CS was shown to have a key space on the order of $10^{83}$, making it extremely resistant to brute force and other types of attacks. \added{In this study, a logistic map was used to encode and construct the sensing matrix. The encryption system exhibited similar key sensitivity and robustness characteristics mentioned in the former.} In the methods above, \added{including those in this study,} sampling was performed in the signal domain (i.e., temporal domain for audio, spatial domain for images), and the reconstruction was performed in the frequency domain.

\deleted{Audio signals, compared to images, have higher information density.} Whereas images are not naturally bandlimited and rather, are dependent on the spatial resolution and bit depth of the imaging device, audio size scales proportionally with time and takes on a wider range of values. The accepted frequency range of human hearing is from 20~Hz to 20~kHz, so by the NST, a sampling frequency \replaced{greater than}{at least} 40~kHz is needed to ensure that an audio sample is recorded \replaced{completely}{correctly}. Any meaningful audio recording, especially those containing speech, will certainly have a \replaced{significant duration}{duration of a few seconds up to a few hours}, so one cannot straightforwardly apply methodologies used for images. The first challenge this would pose for electronic systems is insufficient memory to process the entire signal all at once. Low circumvented this problem \cite{Low2013,Low2018} by transforming the signal to the modulation domain, \deleted{i.e., the signal's spectrogram,} essentially raising a one-dimensional signal to $N$-dimensions, where $N$ is dependent on the desired spectrogram resolution and number of subbands\deleted{, and percent overlap between adjacent subbands}. \added{This method was adopted in this study, and additionally, each subband was multiplied with a window function to suppress potential boundary artifacts when reconstructing. In earlier experiments, reconstruction would often completely fail when windowing was not used. In the cases, however, that were successful, the reconstruction exhibited severe boundary artifacts in the form of distortion, aliasing, or noise.}


\section{Novelty}
\label{sec:novel}
This study aims to provide a generalization for applying CS techniques to signals of arbitrary dimensions\added{, for applications such as compression, encryption, and enhancement}. \replaced{Contemporary}{Previous} CS research work exclusively on either audio or image \replaced{signals}{sequences}\deleted{as the target for CS}, and, due to the computational demands, focus \deleted{of most of the research in the field has been to} \added{on constructing effective sensing matrices,} optimiz\replaced{ing}{e} the computational complexity for real-time applications, and improving \deleted{signal} reconstruction quality. In the establishment of CS methods, two different general frameworks \deleted{to compressively sample signals} arise\deleted{, namely, one-dimensional CS (1DCS) and two-dimensional CS (2DCS)} \added{for image and audio signals}. \deleted{It is shown that an $N$-dimensional signal can be decomposed into factors of one-dimensional and two-dimensional signals, and can be processed using methods appropriate for each type of signal. Furthermore, it is shown that $N$ is bound not only by the type of signals being worked with, but also the computational power of the decoding/decompressing device. In particular, large values of $N$ are useful in encryption, where a signal is first raised to a high dimension in a certain basis, the sensing matrix is derived from another high-dimensional basis, and the result is cast back to either one or two dimensions to yield the encrypted message.}

\added{Furthermore, current research tend to evaluate signal reconstruction quality using statistical metrics, such as mean-squared error (MSE) and its variants. Arguably, the final interpreter of all signals are humans, and it is important to be able to tell how well any compressive/reconstructive algorithm performs just by looking at the metrics without directly observing the signal contents. In light of this, the study also aims to evaluate the reconstruction quality of CS algorithms using perceptually accurate metrics. This class of objective metrics are usually built upon now-obsolete subjective scoring systems, and allows human observers to make an informed estimate of the signal quality without directly accessing the signal itself.}

\added{Finally, this study is conducted in order to lay out a unified, standardized workflow for similar applications of CS on signals with arbitrary content. This includes signals containing a combination of audio and images, such as color videos and hyperspectral images.}


\section{Thesis overview}
\label{sec:overview}
The next chapter establishes the relevant mathematical concepts and notation to be used throughout this study, algorithms used in signal reconstruction, and appropriate metrics per type of signal. \added{Chapter~3 establishes basic workflows and studies the effect of random sampling on CS reconstruction.} Chapters~\replaced{4 \& 5}{3--5} respectively focus on \replaced{image-based CS and audio-based CS}{two-dimensional CS, one-dimensional CS, and $N$-dimensional CS}. Each of these chapters are self-contained methodologies, results, and discussions to emphasize that the methods can work independently of each other. Conclusions of the study and recommendations for future studies are presented in Chapter~6.