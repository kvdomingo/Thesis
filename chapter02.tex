\section{Compressive sensing}
\label{sec:cs}

Consider a signal which we wish to capture. This signal is real-valued and, depending on its dimensionality, can be represented as either a vector or a matrix. The acquisition of this signal can be modeled as a linear system, where the real signal values are transformed into values that can be understood by some sensing device or computer by applying a linear transformation by

\begin{equation}\label{eq:cesa}
    \vec{y} = \innerproduct{\vec{x}}{\vec{A}}
\end{equation}

\noindent where $\vec{A}$ is the transformation commonly known as a sensing matrix.

Consider a discrete time signal $\vec{x} \in \mathbb{R}^N$. Consider also an orthonormal basis $\bm\Psi \in \mathbb{R}^{N \times N}$ whose column vectors are expressed as $\bm\psi_i$. The signal $\vec{x}$ can be expressed as

\begin{equation}\label{eq:signal-transform}
	\vec{x} = \innerproduct{\bm\alpha}{\bm\Psi}
\end{equation}

\noindent where $\alpha_i$ are the coefficients of $\vec{x}$ in the $\bm\Psi$ domain. For practical signal processing applications, a sampled signal is usually of lower dimension than its original representation. We define the compressed vector $\vec{y} \in \mathbb{R}^M$, $M < N$ which is obtained by

\noindent where $\vec{A} \in \mathbb{R}^{M \times N}$ ($M \leq N$) is the sensing matrix. According to the NST, a periodic signal---which may be composed of a linear superposition of sinusoids with different parameters---has a characteristic bandwidth $f_B$, defined by its highest frequency component. The signal can be successfully reconstructed by a sampling device if the signal is sampled at a fixed rate with frequency $f_s$, which is at least twice $f_B$; that is $f_s \geq 2f_B$, where $2f_B$ is known as the Nyquist rate \cite{Shannon1949}. Under the NST, the original signal $\vec{x}$ in \eqref{eq:cesa} can be recovered by inversion, least squares approximation, or more commonly, sinc interpolation. More often than not, the inversion of \eqref{eq:cesa} is ill-posed because it is an underdetermined system. However, recovery is still possible provided that $\vec{x}$ is sparse or can be represented sparsely in some domain, and $\vec{A}$ satisfies one of the following properties \cite{Mazumdar2015}:
\begin{itemize}
	\item \textbf{Low mutual coherence}: The mutual coherence of a matrix $\vec{A}$ is defined as
	\begin{equation}
		\mu(\vec{A}) = \max_{i \neq j} \abs{\innerproduct{\vec{a}_i}{\vec{a}_j}}
	\end{equation}
	\item \textbf{Restricted isometry property} (RIP): Matrix $\vec{A}$ satisfies the RIP with sparsity parameter $k$ and restricted isometry constant $\delta$ if for all $k$-sparse vectors $\vec{x}$,
	\begin{equation}
		\qty(1 - \delta)\norm{\vec{x}}_2^2 \leq \norm{\vec{Ax}}_2^2 \leq \qty(1 - \delta)\norm{\vec{x}}_2^2
	\end{equation}
	\noindent The sparsity parameter $k$ entails that a vector has, at most, $k$ nonzero coefficients, in which case the vector is said to be $k$-sparse \cite{Candes2006}.
\end{itemize}

\noindent Once these have been satisfied, $\vec{x}$ can be recovered exactly from the compressive measurements $\vec{y}$ by solving the combinatorial minimization problem

\begin{equation}\label{eq:min-l0}
	\min \norm{\vec{x}}_0 \quad \textrm{subject to} \quad \vec{Ax = y}
\end{equation}

\noindent where $\norm{\vec{x}}_0$ denotes the $\ell_0$ pseudo-norm of $\vec{x}$, which extracts the number of nonzero coefficients. However, this problem is computationally intractable even for a small signal. A more tractable solution is to reduce this to a convex problem

\begin{equation}\label{eq:min-l1}
	\min \norm{\vec{x}}_1 \quad \textrm{subject to} \quad \vec{Ax = y}
\end{equation}

\noindent where $\norm{\vec{x}}_1 \equiv \sum_i \abs{x_i}$ denotes the $\ell_1$ norm of $\vec{x}$. For a sufficiently sparse signal, the solutions to \eqref{eq:min-l0} and \eqref{eq:min-l1} are identical \cite{Candes2006a}. A plethora of algorithms exist dedicated to solving \eqref{eq:min-l1}. The scope of this study utilizes the following algorithms used commonly in the literature:

\begin{itemize}
	\item \textbf{Least absolute shrinkage and selection operator} (LASSO): recasts the $\ell_1$ minimization problem as an $\ell_1$-regularized least squares problem:
	\begin{equation}
		\min_{\vec{x}} \frac{1}{2N} \norm{\vec{y} - \vec{Ax}}_2^2 + \alpha \norm{\vec{x}}_1
	\end{equation}
	where $\alpha$ is the regularization hyperparameter. Optimization is performed via coordinate descent \cite{scikit-learn}.
\end{itemize}


\section{Associative memory networks}
\label{sec:amnn}