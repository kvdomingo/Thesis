Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Candes2006,
abstract = {Suppose we wish to recover an n-dimensional real-valued vector x{\_}0 (e.g. a digital signal or image) from incomplete and contaminated observations y = A x{\_}0 + e; A is a n by m matrix with far fewer rows than columns (n {\textless}{\textless} m) and e is an error term. Is it possible to recover x{\_}0 accurately based on the data y? To recover x{\_}0, we consider the solution x* to the l1-regularization problem min $\backslash$|x$\backslash$|{\_}1 subject to $\backslash$|Ax-y$\backslash$|{\_}2 {\textless}= epsilon, where epsilon is the size of the error term e. We show that if A obeys a uniform uncertainty principle (with unit-normed columns) and if the vector x{\_}0 is sufficiently sparse, then the solution is within the noise level $\backslash$|x* - x{\_}0$\backslash$|{\_}2 $\backslash$le C epsilon. As a first example, suppose that A is a Gaussian random matrix, then stable recovery occurs for almost all such A's provided that the number of nonzeros of x{\_}0 is of about the same order as the number of observations. Second, suppose one observes few Fourier samples of x{\_}0, then stable recovery occurs for almost any set of p coefficients provided that the number of nonzeros is of the order of n/[$\backslash$log m]{\^{}}6. In the case where the error term vanishes, the recovery is of course exact, and this work actually provides novel insights on the exact recovery phenomenon discussed in earlier papers. The methodology also explains why one can also very nearly recover approximately sparse signals.},
archivePrefix = {arXiv},
arxivId = {arXiv:math/0503066v2},
author = {Cand{\`{e}}s, Emmanuel J. and Romberg, Justin K. and Tao, Terence},
doi = {10.1002/cpa.20124},
eprint = {0503066v2},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Cand{\`{e}}s, Romberg, Tao - 2006 - Stable signal recovery from incomplete and inaccurate measurements.pdf:pdf},
issn = {00103640},
journal = {Commun. Pure Appl. Math.},
keywords = {a national science foundation,acknowledgments,and by an alfred,basis pursuit,c,dms 01-40698,e,frg,grant,is partially supported by,is supported by national,j,p,r,restricted orthonormality,singular,sloan fellowship,sparsity,values of random matrices,ℓ 1 -minimization},
number = {8},
pages = {1207--1223},
primaryClass = {arXiv:math},
title = {{Stable signal recovery from incomplete and inaccurate measurements}},
volume = {59},
year = {2006}
}
@article{Donoho2001,
author = {Donoho, D.L. and Huo, X.},
doi = {10.1109/18.959265},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Donoho, Huo - 2001 - Uncertainty principles and ideal atomic decomposition.pdf:pdf},
issn = {00189448},
journal = {IEEE Trans. Inf. Theory},
keywords = {Basis pursuit,Logan's phenomenon,combinatorial optimization,convex optimization,error-correcting encryption,harmonic analysis,matching pursuit,multiple-basis signal representation,overcomplete representation,ridgelet analysis,uncertainty principle,wavelet analysis},
month = {aug},
number = {7},
pages = {2845--2862},
title = {{Uncertainty principles and ideal atomic decomposition}},
url = {http://openurl.ingenta.com/content/xref?genre=article{\&}issn=1533-4880{\&}volume=11{\&}issue=8{\&}spage=7382 http://ieeexplore.ieee.org/document/959265/},
volume = {47},
year = {2001}
}
@article{Lecun2015,
abstract = {This article presents results from an international collaboration between college students and pre-service teachers in Norway and the UK. This research is part of a large, international project exploring and developing the interrelationship between mobile technology and teachers' perceptions of teaching and learning. Data was collected for this study through an on-line survey of 37 pre-service teachers followed by six semi-structured, in-depth interviews. The data analysis revealed the themes of collaboration, authenticity and professional learning through the use of mobile technology in the data. The collaboration enabled the use of the affordances of mobile technology to enhance the pre-service teachers' professional learning and the data suggested that this enhanced their emergent conceptions of teaching and learning.},
author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Lecun, Bengio, Hinton - 2015 - Deep learning.pdf:pdf},
issn = {14764687},
journal = {Nature},
number = {7553},
pages = {436--444},
title = {{Deep learning}},
volume = {521},
year = {2015}
}
@article{Harms2011,
abstract = {Technological constraints severely limit the rate at which analog-to-digital converters can reliably sample signals. Recently, Tropp et al. proposed an architecture, termed the random demodulator (RD), that attempts to overcome this obstacle for sparse bandlimited signals. One integral component of the RD architecture is a white noise-like, bipolar modulating waveform that changes polarity at a rate equal to the signal bandwidth. Since there is a hardware limitation to how fast analog waveforms can change polarity without undergoing shape distortion, this leads to the RD also having a constraint on the maximum allowable bandwidth. In this paper, an extension of the RD, termed the constrained random demodulator (CRD), is proposed that bypasses this bottleneck by replacing the original modulating waveform with a run-length limited (RLL) modulating waveform that changes polarity at a slower rate than the signal bandwidth. One of the main contributions of the paper is establishing that the CRD, despite employing a modulating waveform with correlations, enjoys some theoretical guarantees for certain RLL waveforms. In addition, for a given sampling rate and rate of change in the modulating waveform polarity, numerical simulations confirm that the CRD, using an appropriate RLL waveform, can sample a signal with an even wider bandwidth without a significant loss in performance.},
author = {Harms, Andrew and Bajwa, Waheed U. and Calderbank, Robert},
doi = {10.1109/ICASSP.2011.5947721},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Harms, Bajwa, Calderbank - 2011 - Beating Nyquist through correlations A constrained random demodulator for sampling of sparse bandlimit.pdf:pdf},
isbn = {9781457705397},
issn = {15206149},
journal = {ICASSP, IEEE Int. Conf. Acoust. Speech Signal Process. - Proc.},
pages = {5968--5971},
title = {{Beating Nyquist through correlations: A constrained random demodulator for sampling of sparse bandlimited signals}},
year = {2011}
}
@article{Shi2019,
author = {Shi, Wuzhen and Jiang, Feng and Liu, Shaohui and Zhao, Debin},
doi = {10.1109/tip.2019.2928136},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Shi et al. - 2019 - Image Compressed Sensing using Convolutional Neural Network.pdf:pdf},
issn = {1057-7149},
journal = {IEEE Trans. Image Process.},
number = {XX},
pages = {1--1},
publisher = {IEEE},
title = {{Image Compressed Sensing using Convolutional Neural Network}},
volume = {PP},
year = {2019}
}
@article{Xie2019,
abstract = {Neural networks for image recognition have evolved through extensive manual design from simple chain-like models to structures with multiple wiring paths. The success of ResNets and DenseNets is due in large part to their innovative wiring plans. Now, neural architecture search (NAS) studies are exploring the joint optimization of wiring and operation types, however, the space of possible wirings is constrained and still driven by manual design despite being searched. In this paper, we explore a more diverse set of connectivity patterns through the lens of randomly wired neural networks. To do this, we first define the concept of a stochastic network generator that encapsulates the entire network generation process. Encapsulation provides a unified view of NAS and randomly wired networks. Then, we use three classical random graph models to generate randomly wired graphs for networks. The results are surprising: several variants of these random generators yield network instances that have competitive accuracy on the ImageNet benchmark. These results suggest that new efforts focusing on designing better network generators may lead to new breakthroughs by exploring less constrained search spaces with more room for novel design.},
archivePrefix = {arXiv},
arxivId = {1904.01569},
author = {Xie, Saining and Kirillov, Alexander and Girshick, Ross and He, Kaiming},
eprint = {1904.01569},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Xie et al. - 2019 - Exploring Randomly Wired Neural Networks for Image Recognition.pdf:pdf},
title = {{Exploring Randomly Wired Neural Networks for Image Recognition}},
url = {http://arxiv.org/abs/1904.01569},
year = {2019}
}
@article{Cross2008,
abstract = {The physics of falling from a height, a topic that could be included in a course on forensic physics or in an undergraduate class as an example of Newton's laws, is applied to a common forensic problem. {\textcopyright} 2008 American Association of Physics Teachers.},
author = {Cross, Rod},
doi = {10.1119/1.2919736},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Cross - 2008 - Forensic Physics 101 Falls from a height.pdf:pdf},
issn = {0002-9505},
journal = {Am. J. Phys.},
number = {9},
pages = {833--837},
title = {{Forensic Physics 101: Falls from a height}},
volume = {76},
year = {2008}
}
@article{Mo2013,
abstract = {An image compression-encryption algorithm based on 2-D compressive sensing is proposed, which can accomplish encryption and compression simultaneously. The measurements are performed in two directi-ons and the measurement matrices are constructed as partial Hadamard matrices, which are controlled by chaos map. The proposed algorithm is sensitive to the keys and it can resist various attacks. Simulation results verify the validity and reliability of the proposed algorithm. Copyright {\textcopyright} 2013 Binary Information Press.},
author = {Mo, Yan and Zhang, Aidi and Zheng, Fen and Zhou, Nanrun},
doi = {10.12733/jcis8608},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Mo et al. - 2013 - An image compression-encryption algorithm based on 2-D compressive sensing.pdf:pdf},
issn = {15539105},
journal = {J. Comput. Inf. Syst.},
keywords = {Compressive sensing,Image compression,Image encryption},
number = {24},
pages = {10057--10064},
title = {{An image compression-encryption algorithm based on 2-D compressive sensing}},
volume = {9},
year = {2013}
}
@article{Romero2015,
author = {Romero, Roland Albert A and Tapang, Giovanni A},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Romero, Tapang - 2015 - Compressive sampling on hologram phase maps.pdf:pdf},
keywords = {40,42,compressive sampling,hologram phase maps,jv},
number = {June},
pages = {1--4},
title = {{Compressive sampling on hologram phase maps}},
year = {2015}
}
@article{Wu2019,
abstract = {Compressed sensing (CS) provides an elegant framework for recovering sparse signals from compressed measurements. For example, CS can exploit the structure of natural images and recover an image from only a few random measurements. CS is flexible and data efficient, but its application has been restricted by the strong assumption of sparsity and costly reconstruction process. A recent approach that combines CS with neural network generators has removed the constraint of sparsity, but reconstruction remains slow. Here we propose a novel framework that significantly improves both the performance and speed of signal recovery by jointly training a generator and the optimisation process for reconstruction via meta-learning. We explore training the measurements with different objectives, and derive a family of models based on minimising measurement errors. We show that Generative Adversarial Nets (GANs) can be viewed as a special case in this family of models. Borrowing insights from the CS perspective, we develop a novel way of improving GANs using gradient information from the discriminator.},
archivePrefix = {arXiv},
arxivId = {1905.06723},
author = {Wu, Yan and Rosca, Mihaela and Lillicrap, Timothy},
eprint = {1905.06723},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Wu, Rosca, Lillicrap - 2019 - Deep Compressed Sensing.pdf:pdf},
title = {{Deep Compressed Sensing}},
url = {http://arxiv.org/abs/1905.06723},
year = {2019}
}
@article{Aghajari2015,
abstract = {In this study, a novel hetero-associative memory with dynamic behavior is proposed. The proposed hetero-associative memory can store as twice as a regular hetero-associative memory using a new extension of sparse learning method. The new learning method gives the network ability of successive learning, therefore it can store new patterns even after learning phase. In other words, learning step and recall step are not separated in this method. We also add chaos searching in recall step in order to make the network be able to converge into the best possible solution among whole search space. Chaotic behavior helps the network jumps from local minimums. Simulation result shows higher storage capacity and also better recall performance in comparison with regular hetero-associative memory with the presence of noisy input data.},
author = {Aghajari, Z. H. and Teshnehlab, M. and {Jahed Motlagh}, M. R.},
doi = {10.1016/j.neucom.2015.04.060},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Aghajari, Teshnehlab, Jahed Motlagh - 2015 - A novel chaotic hetero-associative memory.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Associative memory,Chaos,Hetero-associative memory,Neural network},
pages = {352--358},
publisher = {Elsevier},
title = {{A novel chaotic hetero-associative memory}},
url = {http://dx.doi.org/10.1016/j.neucom.2015.04.060},
volume = {167},
year = {2015}
}
@article{Low2013,
abstract = {This paper presents an alternative approach to speech enhancement by using compressed sensing (CS). CS is a new sampling theory, which states that sparse signals can be reconstructed from far fewer measurements than the Nyquist sampling. As such, CS can be exploited to reconstruct only the sparse components (e.g., speech) from the mixture of sparse and non-sparse components (e.g., noise). This is possible because in a time-frequency representation, speech signal is sparse whilst most noise is non-sparse. Derivation shows that on average the signal to noise ratio (SNR) in the compressed domain is greater or equal than the uncompressed domain. Experimental results concur with the derivation and the proposed CS scheme achieves better or similar perceptual evaluation of speech quality (PESQ) scores and segmental SNR compared to other conventional methods in a wide range of input SNR. {\textcopyright} 2013 Elsevier B.V. All rights reserved.},
author = {Low, Siow Yong and Pham, Duc Son and Venkatesh, Svetha},
doi = {10.1016/j.specom.2013.03.003},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Low, Pham, Venkatesh - 2013 - Compressive speech enhancement.pdf:pdf},
issn = {01676393},
journal = {Speech Commun.},
keywords = {Compressed sensing,Sparsity,Speech enhancement},
number = {6},
pages = {757--768},
publisher = {Elsevier B.V.},
title = {{Compressive speech enhancement}},
url = {http://dx.doi.org/10.1016/j.specom.2013.03.003},
volume = {55},
year = {2013}
}
@article{Warbhe2016,
abstract = {It is crucial in image forensics to prove the authenticity of the digital images. Due to the availability of the using sophisticated image editing software programs, anyone can manipulate the images easily. There are various types of digital image manipulation or tampering possible; like image compositing, splicing, copy-paste, etc. In this paper, we propose a passive scaling robust algorithm for the detection of Copy-Paste tampering. Sometimes the copied region of an image is scaled before pasting to some other location in the image. In such cases, the normal Copy-Paste detection algorithm fails to detect the forgeries. We have implemented and used an improved customized Normalized Cross Correlation for detecting highly correlated areas from the image and the image blocks, thereby detecting the tampered regions from an image. The experimental results demonstrate that the proposed approach can be effectively used to detect copy-paste forgeries accurately and is scaling robust.},
author = {Warbhe, Anil Dada and Dharaskar, R. V. and Thakare, V. M.},
doi = {10.1016/j.procs.2016.03.059},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Warbhe, Dharaskar, Thakare - 2016 - A Scaling Robust Copy-Paste Tampering Detection for Digital Image Forensics.pdf:pdf},
issn = {18770509},
journal = {Procedia Comput. Sci.},
keywords = {Digital Image Forensics,Image Authentication,Image Forgery Detection,Image Tampering},
pages = {458--465},
publisher = {Elsevier Masson SAS},
title = {{A Scaling Robust Copy-Paste Tampering Detection for Digital Image Forensics}},
url = {http://dx.doi.org/10.1016/j.procs.2016.03.059},
volume = {79},
year = {2016}
}
@article{Stamm2011,
abstract = {As society has become increasingly reliant upon digital images to communicate visual information, a number of forensic techniques have been developed to verify the authenticity of digital images. Amongst the most successful of these are techniques that make use of an image's compression history and its associated compression fingerprints. Little consideration has been given, however, to anti-forensic techniques capable of fooling forensic algorithms. In this paper, we present a set of anti-forensic techniques designed to remove forensically significant indicators of compression from an image. We do this by first developing a generalized framework for the design of anti-forensic techniques to remove compression fingerprints from an image's transform coefficients. This framework operates by estimating the distribution of an image's transform coefficients before compression, then adding anti-forensic dither to the transform coefficients of a compressed image so that their distribution matches the estimated one. We then use this framework to develop anti-forensic techniques specifically targeted at erasing compression fingerprints left by both JPEG and wavelet-based coders. Additionally, we propose a technique to remove statistical traces of the blocking artifacts left by image compression algorithms that divide an image into segments during processing. Through a series of experiments, we demonstrate that our anti-forensic techniques are capable of removing forensically detectable traces of image compression without significantly impacting an image's visual quality. Furthermore, we show how these techniques can be used to render several forms of image tampering such as double JPEG compression, cut-and-paste image forgery, and image origin falsification undetectable through compression-history-based forensic means.},
author = {Stamm, Matthew C. and Liu, K.J. Ray},
doi = {10.1109/TIFS.2011.2119314},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Stamm, Liu - 2011 - Anti-forensics of digital image compression.pdf:pdf},
issn = {1556-6013},
journal = {IEEE Trans. Inf. Forensics Secur.},
keywords = {Anti-forensics,JPEG compression,anti-forensic dither,digital forensics,image compression},
month = {sep},
number = {3},
pages = {1050--1065},
title = {{Anti-forensics of digital image compression}},
url = {http://ieeexplore.ieee.org/document/5720310/},
volume = {6},
year = {2011}
}
@article{Andrychowicz2016,
abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
archivePrefix = {arXiv},
arxivId = {arXiv:1606.04474v2},
author = {Andrychowicz, Marcin and Denil, Misha and Colmenarejo, Sergio G{\'{o}}mez and Hoffman, Matthew W. and Pfau, David and Schaul, Tom and Shillingford, Brendan and {De Freitas}, Nando},
eprint = {arXiv:1606.04474v2},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Andrychowicz et al. - 2016 - Learning to learn by gradient descent by gradient descent.pdf:pdf},
issn = {10495258},
journal = {Adv. Neural Inf. Process. Syst.},
number = {Nips},
pages = {3988--3996},
title = {{Learning to learn by gradient descent by gradient descent}},
year = {2016}
}
@article{Rubinstein2008,
abstract = {The K-SVD algorithm is a highly effective method of training overcomplete dic- tionaries for sparse signal representation. In this report we discuss an efficient im- plementation of this algorithm, which both accelerates it and reduces its memory consumption. The two basic components of our implementation are the replacement of the exact SVD computation with a much quicker approximation, and the use of the Batch-OMP method for performing the sparse-coding operations. Batch-OMP, which we also present in this report, is an implementation of the Orthogonal Matching Pursuit (OMP) algorithm which is specifically optimized for sparse-coding large sets of signals over the same dictionary. The Batch-OMP imple- mentation is useful for a variety of sparsity-based techniques which involve coding large numbers of signals. In the report, we discuss the Batch-OMP and K-SVD implementations and analyze their complexities. The report is accompanied by Matlab toolboxes which implement these techniques, and can be downloaded at http://www.cs.technion.ac.il/{\~{}}ronrubin/software.html.},
author = {Rubinstein, Ron and Zibulevsky, Michael and Elad, Michael},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Rubinstein, Zibulevsky, Elad - 2008 - Efficient implementation of the K-SVD algorithm using batch orthogonal matching pursuit.pdf:pdf},
journal = {CS Tech.},
pages = {1--15},
title = {{Efficient implementation of the K-SVD algorithm using batch orthogonal matching pursuit}},
url = {http://cs.technion.ac.il/users/wwwb/cgi-bin/tr-get.cgi/2008/CS/CS-2008-08.revised.pdf},
year = {2008}
}
@article{Andras2018,
abstract = {In this paper stochastic sampling as a method of frequency sparse signal acquisition is presented. Basic principle of compressed sensing is reviewed, with emphasis on nonuniform sampling and signal reconstruction methods. A robust time domain reconstruction method of randomly sampled signal through compressed sensing approach is proposed. The presented reconstruction algorithm is evaluated by means of simulations, with comparison to conventional compressed sensing reconstruction and the most common practical issues taken into account. Simulation results indicate that the proposed reconstruction method is resistent to high levels of quantization and uncorrelated noise. Experiments with real hardware were also performed, results of which confirm the ability of stochastic sampling framework to overcome the Nyquist limit of analog-to-digital converters.},
author = {Andr{\'{a}}{\v{s}}, Imrich and Dolinsk{\'{y}}, Pavol and Michaeli, Linus and {\v{S}}aliga, J{\'{a}}n},
doi = {10.1016/j.measurement.2018.05.065},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Andr{\'{a}}{\v{s}} et al. - 2018 - A time domain reconstruction method of randomly sampled frequency sparse signal.pdf:pdf},
issn = {02632241},
journal = {Meas. J. Int. Meas. Confed.},
keywords = {Analog-to-information conversion,Compressed sensing,Nonuniform sampling,Sparse signal,Stochastic sampling},
number = {December 2016},
pages = {68--77},
title = {{A time domain reconstruction method of randomly sampled frequency sparse signal}},
volume = {127},
year = {2018}
}
@article{Hopfield1982,
abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
author = {Hopfield, J J},
doi = {10.1073/pnas.79.8.2554},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Hopfield - 1982 - Neural networks and physical systems with emergent collective computational abilities.pdf:pdf},
journal = {Proc. Natl. Acad. Sci. U. S. A.},
number = {8},
pages = {2554--2558},
title = {{Neural networks and physical systems with emergent collective computational abilities}},
volume = {79},
year = {1982}
}
@article{Sankar2019,
abstract = {A novel scalable speech coding scheme based on Compressive Sensing (CS), which can operate at bit rates from 3.275 to 7.275 kbps is designed and implemented in this paper. The CS based speech coding offers the benefit of combined compression and encryption with inherent de-noising and bit rate scalability. The non-stationary nature of speech signal causes the recovery process from CS measurements very complex due to the variation in sparsifying bases. In this work, the complexity of the recovery process is reduced by assigning a suitable basis to each frame of the speech signal based on its statistical properties. As the quality of the reconstructed speech depends on the sensing matrix used at the transmitter, a variant of Binary Permuted Block Diagonal (BPBD) matrix is also proposed here which offers a better performance than that of the commonly used Gaussian random matrix. To improve the coding efficiency, formant filter coefficients are quantized using the conventional Vector Quantization (VQ) and an orthogonal mapping based VQ is developed for the quantization of CS measurements. The proposed coding scheme offers the listening quality for reconstructed speech similar to that of Adaptive Multi rate - Narrowband (AMR-NB) codec at 6.7 kbps and Enhanced Voice Services (EVS) at 7.2 kbps. A separate de-noising block is not required in the proposed coding scheme due to the inherent de-noising property of CS. Scalability in bit rate is achieved in the proposed method by varying the number of random measurements and the number of levels for orthogonal mapping in the VQ stage of measurements.},
author = {Sankar, M. S.Arun and Sathidevi, P. S.},
doi = {10.1016/j.heliyon.2019.e01820},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Sankar, Sathidevi - 2019 - A scalable speech coding scheme using compressive sensing and orthogonal mapping based vector quantization.pdf:pdf},
issn = {24058440},
journal = {Heliyon},
keywords = {CELP,Compressive sensing,Electrical engineering,Speech coding,Speech compression,Speech processing,Wavelet},
number = {5},
pages = {e01820},
publisher = {Elsevier Ltd},
title = {{A scalable speech coding scheme using compressive sensing and orthogonal mapping based vector quantization}},
url = {https://doi.org/10.1016/j.heliyon.2019.e01820},
volume = {5},
year = {2019}
}
@article{LinhTrung2008,
abstract = {Compressed sensing, viewed as a type of random undersampling, considers the acquisition and reconstruction of sparse or compressible signals at a rate significantly lower than that of Nyquist. Exact reconstruction from incompletely acquired random measurements is, under certain constraints, achievable with high probability. However, randomness may not always be desirable in certain applications. Taking a nonrandom approach using deterministic chaos and following closely a recently proposed novel efficient structure of chaos filters, we propose a chaos filter structure by exploring the use of chaotic deterministic processes in designing the filter taps. By numerical performance, we show that, chaos filters generated by the logistic map, while being possible to exactly reconstruct original timesparse signals from their incompletely acquired measurements, outperforms random filters.},
author = {Linh-Trung, Nguyen and {Van Phong}, Dinh and Hussain, Zahir M. and Huynh, Huu Tue and Morgan, Victoria L. and Gore, John C.},
doi = {10.1109/ATNAC.2008.4783326},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Linh-Trung et al. - 2008 - Compressed sensing using chaos filters.pdf:pdf},
isbn = {9781424426034},
journal = {Proc. 2008 Australas. Telecommun. Networks Appl. Conf. ATNAC 2008},
keywords = {Chaos filters,Chaotic undersampling,Compressed sensing,Random filters,Random undersampling},
pages = {219--223},
title = {{Compressed sensing using chaos filters}},
year = {2008}
}
@article{Lin2009,
abstract = {Recent development in multimedia processing and network technologies has facilitated the distribution and sharing of multimedia through networks, and increased the security demands of multimedia contents. Traditional image content protection schemes use extrinsic approaches, such as watermarking or fingerprinting. However, under many circumstances, extrinsic content protection is not possible. Therefore, there is great interest in developing forensic tools via intrinsic fingerprints to solve these problems. Source coding is a common step of natural image acquisition, so in this paper, we focus on the fundamental research on digital image source coder forensics via intrinsic fingerprints. First, we investigate the unique intrinsic fingerprint of many popular image source encoders, including transform-based coding (both discrete cosine transform and discrete wavelet transform based), subband coding, differential image coding, and also block processing as the traces of evidence. Based on the intrinsic fingerprint of image source encoders, we construct an image source coding forensic detector that identifies which source encoder is applied, what the coding parameters are along with confidence measures of the result. Our simulation results show that the proposed system provides trustworthy performance: for most test cases, the probability of detecting the correct source encoder is over 90{\%}.},
author = {Lin, W. Sabrina and Tjoa, Steven K. and Zhao, H. Vicky and Liu, K. J.Ray},
doi = {10.1109/TIFS.2009.2024715},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Lin et al. - 2009 - Digital image source coder forensics via intrinsic fingerprints.pdf:pdf},
issn = {15566013},
journal = {IEEE Trans. Inf. Forensics Secur.},
keywords = {Image source coding,Intrinsic fingerprint,Multimedia forensics},
number = {3},
pages = {460--475},
title = {{Digital image source coder forensics via intrinsic fingerprints}},
volume = {4},
year = {2009}
}
@article{Esmi2016,
abstract = {This paper introduces a new class of fuzzy associative memories (FAMs) called tunable equivalence fuzzy associative memories, for short tunable E-FAMs or TE-FAMs, that are determined by the application of parametrized equivalence measures in the hidden nodes. Tunable E-FAMs belong to the class of $\Theta$-FAMs that have recently appeared in the literature. In contrast to previous $\Theta$-FAM models, tunable E-FAMs allow for the extraction of a fundamental memory set from the training data by means of an algorithm that depends on the evaluation of equivalence measures. Furthermore, we are able to optimize not only the weights corresponding to the contributions of the hidden nodes but also the contributions of the attributes of the data by tuning the parametrized equivalence measures used in a TE-FAM model. The computational effort involved in training tunable TE-FAMs is very low compared to the one of the previous $\Theta$-FAM training algorithm.},
author = {Esmi, Estev{\~{a}}o and Sussner, Peter and Sandri, Sandra},
doi = {10.1016/j.fss.2015.04.004},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Esmi, Sussner, Sandri - 2016 - Tunable equivalence fuzzy associative memories.pdf:pdf},
issn = {01650114},
journal = {Fuzzy Sets Syst.},
keywords = {Classification,Fuzzy associative memory,Parametrized equivalence measure,Selection of fundamental memories,Supervised learning,Tunable equivalence fuzzy associative memory},
pages = {242--260},
publisher = {Elsevier B.V.},
title = {{Tunable equivalence fuzzy associative memories}},
url = {http://dx.doi.org/10.1016/j.fss.2015.04.004},
volume = {292},
year = {2016}
}
@article{Zhao2019,
abstract = {Recent years a variety of CNN-based (convolutional neural network) approaches for compressive sensing (CS) have been proposed. They learn a transform to recover the original signals from the measurements obtained by measuring the scene at a sub-Nyquist sampling rate. Among them, the LMM-based ones (learned measurement matrix) exhibit better performance. In this paper, we visualize the LMM-based CS framework. This is the first time an insight look is taken into the CS network. It helps us understand how CS framework works. Taking the proposed LMM-based framework as an example, where reasonable residual blocks in the recovery part let it achieve excellent performance over the existing ones, we analyze the mechanism of CNN-based CS by the visualization. In the measurement part, intuitive representation of the measurement matrices is presented. As for the recovery procedure, an explanation of the preliminary recovery is given from the viewpoints of system and space. We analyze how the residual block adds the mainly high-frequency information. Through the comparison of the visualization of the typical methods, it is explored that the measurement and recovery part of the proposed method can promote each other, and the learned CS framework with residual network achieves the best performance. In addition, a set of experiments are conducted on a standard dataset to verify the better performance of our framework.},
author = {Zhao, Zhifu and Xie, Xuemei and Wang, Chenye and Liu, Wan and Shi, Guangming and Du, Jiang},
doi = {10.1016/j.neucom.2019.05.043},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Zhao et al. - 2019 - Visualizing and understanding of learned compressive sensing with residual network.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Compressive sensing,Learned measurement matrix,Residual network,Visualizing and understanding},
pages = {185--198},
publisher = {Elsevier B.V.},
title = {{Visualizing and understanding of learned compressive sensing with residual network}},
volume = {359},
year = {2019}
}
@article{Yanwei2015,
abstract = {Signal sampling is a vital component in modern information technology. As the signal bandwidth becomes wider, the sampling rate of analog-to-digital conversion (ADC) based on Shannon-Nyquist theorem is more and more high and may be beyond its capacity. However the analog to information converter (AIC) based on compressed sensing (CS) is designed to sample the analog signals at a sub-Nyquist sampling rate. A new multi-rate sub-Nyquist sampling (MSS) system was proposed in this article, it has one mixer, one integrator and several parallel ADCs with different sampling rates. Simulation shows the signals can be reconstructed in high probability even though the sampling rate is much lower than the Nyquist sampling rate.},
author = {Yanwei, Xiong and Jianhua, Zhang and Ping, Zhang},
doi = {10.1016/S1005-8885(15)60644-6},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Yanwei, Jianhua, Ping - 2015 - Compressed sensing based multi-rate sub-Nyquist sampling system.pdf:pdf},
issn = {10058885},
journal = {J. China Univ. Posts Telecommun.},
keywords = {AIC,CS,MSS,sub-Nyquist rate},
number = {2},
pages = {89--95},
publisher = {The Journal of China Universities of Posts and Telecommunications},
title = {{Compressed sensing based multi-rate sub-Nyquist sampling system}},
url = {http://dx.doi.org/10.1016/S1005-8885(15)60644-6},
volume = {22},
year = {2015}
}
@article{Shi2018,
abstract = {The regularized least squares for sparse reconstruction is gaining popularity as it has the ability to reconstruct speech signal from a noisy observation. The reconstruction relies on the sparsity of speech, which provides the demarcation from noise. However, there is no measure incorporated in the sparse reconstruction to optimize on the overall speech quality. This paper proposes a two-level optimization strategy to incorporate the quality design attributes in the sparse solution in compressive speech enhancement by hyper-parameterizing the tuning parameter. The first level involves the compression of the big data and the second level optimizes the tuning parameter by using different optimization criteria (such as Gini index, the Akaike information criterion (AIC) and Bayesian information criterion (BIC)). The set of solutions can then be measured against the desired design attributes to achieve the best trade-off between suppression and distortion. Numerical results show the proposed approach can effectively fuse the trade-offs in the solutions for different noise profile in a wide range of signal to noise ratios (SNR).},
author = {Shi, Yue and Low, Siow Yong and {Cedric Yiu}, Ka Fai},
doi = {10.1016/j.apacoust.2018.03.020},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Shi, Low, Cedric Yiu - 2018 - Hyper-parameterization of sparse reconstruction for speech enhancement.pdf:pdf},
issn = {1872910X},
journal = {Appl. Acoust.},
keywords = {Compressed sensing,Regularized least squares,Speech enhancement},
number = {March},
pages = {72--79},
publisher = {Elsevier},
title = {{Hyper-parameterization of sparse reconstruction for speech enhancement}},
url = {https://doi.org/10.1016/j.apacoust.2018.03.020},
volume = {138},
year = {2018}
}
@article{Kettisen2018,
abstract = {Redox active cysteine residues including $\beta$Cys93 are part of hemoglobin's “oxidation hotspot”. Irreversible oxidation of $\beta$Cys93 ultimately leads to the collapse of the hemoglobin structure and release of heme. Human fetal hemoglobin (HbF), similarly to the adult hemoglobin (HbA), carries redox active $\gamma$Cys93 in the vicinity of the heme pocket. Site-directed mutagenesis has been used in this study to examine the impact of removal and/or addition of cysteine residues in HbF. The redox activities of the recombinant mutants were examined by determining the spontaneous autoxidation rate, the hydrogen peroxide induced ferric to ferryl oxidation rate, and irreversible oxidation of cysteine by quantitative mass spectrometry. We found that substitution of $\gamma$Cys93Ala resulted in oxidative instability characterized by increased oxidation rates. Moreover, the addition of a cysteine residue at $\alpha$19 on the exposed surface of the $\alpha$-chain altered the regular electron transfer pathway within the protein by forming an alternative oxidative site. This may also create an accessible site for di-sulfide bonding between Hb subunits. Engineering of cysteine residues at suitable locations may be useful as a tool for managing oxidation in a protein, and for Hb, a way to stave off oxidation reactions resulting in a protein structural collapse.},
author = {Kettisen, Karin and Strader, Michael Brad and Wood, Francine and Alayash, Abdu I. and B{\"{u}}low, Leif},
doi = {10.1016/j.redox.2018.08.010},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Kettisen et al. - 2018 - Site-directed mutagenesis of cysteine residues alters oxidative stability of fetal hemoglobin.pdf:pdf},
issn = {22132317},
journal = {Redox Biol.},
keywords = {Cysteine,Fetal hemoglobin,Hydrogen peroxide,Oxidation,Protein electron transfer,Site-directed mutagenesis},
number = {July},
pages = {218--225},
publisher = {Elsevier B.V.},
title = {{Site-directed mutagenesis of cysteine residues alters oxidative stability of fetal hemoglobin}},
url = {https://doi.org/10.1016/j.redox.2018.08.010},
volume = {19},
year = {2018}
}
@article{Kobayashi2017,
abstract = {A Hopfield associative memory (HAM) is a major model of associative memory using neural networks. The classical HAM model has a low storage capacity. A pseudo-orthogonalized HAM (POHAM) was therefore proposed to improve storage capacity by encoding the training patterns to be pseudo-orthogonal. In the present work, we examine a different property of POHAM. A chaotic HAM (CHAM) can explore embedded patterns, including training patterns. Although it is not desirable to recall patterns other than training patterns, at a minimum, the reversed patterns are recalled. A POHAM regards the training and reversed patterns as equivalent. To take advantage of this property, we propose a chaotic POHAM (CPOHAM). We evaluated a CHAM and a CPOHAM using computer simulations. The CPOHAM never recalled the reversed patterns. In addition, the CPOHAM recalled very few other pseudo-memories, such as mixture patterns, due to the orthogonality of the encoded training patterns, while the CHAM frequently recalled the mixture patterns.},
author = {Kobayashi, Masaki},
doi = {10.1016/j.neucom.2017.02.037},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Kobayashi - 2017 - Chaotic pseudo-orthogonalized Hopfield associative memory.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Chaotic neural network,Hopfield associative memory,Pseudo-orthogonalization,Reversed pattern},
pages = {147--151},
publisher = {Elsevier B.V.},
title = {{Chaotic pseudo-orthogonalized Hopfield associative memory}},
url = {http://dx.doi.org/10.1016/j.neucom.2017.02.037},
volume = {241},
year = {2017}
}
@article{Luo2019,
abstract = {Adaptive optimization methods such as AdaGrad, RMSprop and Adam have been proposed to achieve a rapid training process with an element-wise scaling term on learning rates. Though prevailing, they are observed to generalize poorly compared with SGD or even fail to converge due to unstable and extreme learning rates. Recent work has put forward some algorithms such as AMSGrad to tackle this issue but they failed to achieve considerable improvement over existing methods. In our paper, we demonstrate that extreme learning rates can lead to poor performance. We provide new variants of Adam and AMSGrad, called AdaBound and AMSBound respectively, which employ dynamic bounds on learning rates to achieve a gradual and smooth transition from adaptive methods to SGD and give a theoretical proof of convergence. We further conduct experiments on various popular tasks and models, which is often insufficient in previous work. Experimental results show that new variants can eliminate the generalization gap between adaptive methods and SGD and maintain higher learning speed early in training at the same time. Moreover, they can bring significant improvement over their prototypes, especially on complex deep networks. The implementation of the algorithm can be found at https://github.com/Luolc/AdaBound .},
archivePrefix = {arXiv},
arxivId = {1902.09843},
author = {Luo, Liangchen and Xiong, Yuanhao and Liu, Yan and Sun, Xu},
eprint = {1902.09843},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Luo et al. - 2019 - Adaptive gradient methods with dynamic bound of learning rate.pdf:pdf},
journal = {Int. Conf. Learn. Represent.},
month = {feb},
title = {{Adaptive Gradient Methods with Dynamic Bound of Learning Rate}},
url = {https://openreview.net/forum?id=Bkg3g2R9FX http://arxiv.org/abs/1902.09843},
year = {2019}
}
@article{Lin2016,
abstract = {Bacillus licheniformis $\gamma$-glutamyltranspeptidase (BlGGT) belongs to N-terminal nucleophile hydrolase superfamily in which all inclusive members are synthetized as single-chain precursors, and then self-processed to form mature enzymes. Here we investigated the role of a conserved Asn450 residue in BlGGT through site-directed mutagenesis and molecular characterization of four relevant variants. Substitution of Asn450 by arginine resulted in a significant reduction in the catalytic activity of BlGGT. Conversely, N450A and N450D displayed an enhanced activity. The catalytic efficiency of BlGGT was calculated to be 16.04 mM-1 s-1, but this value was either decreased to 8.93 mM-1 s-1 in N450 K or increased to more than 123.65 mM-1 s-1 in N450A and N450D. In addition, the ratio of transpeptidation to hydrolysis was increased from 3.5 to more than 7.6 by the mutations. Structural analyses showed that fluorescence, circular dichroism spectra and thermal denaturation profiles of mutant proteins were essentially consistent with those of BlGGT. However, guanidine hydrochloride (GdnHCl)-induced transition was significantly reduced in comparison with the wild-type enzyme. Molecular modeling suggests that residue Asn450 of BlGGT is important to create suitable environments for both autoprocessing and catalytic reactions.},
author = {Lin, Min Guan and Chi, Meng Chun and Chen, Yu Yi and Wang, Tzu Fan and Lo, Hui Fen and Lin, Long Liu},
doi = {10.1016/j.ijbiomac.2016.05.101},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Lin et al. - 2016 - Site-directed mutagenesis of a conserved Asn450 residue of Bacillus licheniformis $\gamma$-glutamyltranspeptidase.pdf:pdf},
issn = {18790003},
journal = {Int. J. Biol. Macromol.},
keywords = {Asparagine,Site-directed mutagenesis,$\gamma$-Glutamyltranspeptidase},
pages = {416--425},
publisher = {Elsevier B.V.},
title = {{Site-directed mutagenesis of a conserved Asn450 residue of Bacillus licheniformis $\gamma$-glutamyltranspeptidase}},
url = {http://dx.doi.org/10.1016/j.ijbiomac.2016.05.101},
volume = {91},
year = {2016}
}
@article{Liu2016,
abstract = {Since compressed sensing was introduced in 2006, ℓ1-ℓ2 minimization admits a large number of applications in signal processing, statistical inference, magnetic resonance imaging (MRI), computed tomography (CT), etc. In this paper, we present a neural network for ℓ1-ℓ2 minimization based on scaled gradient projection. We prove that it is stable in the sense of Lyapunov and converges to an optimal solution of the ℓ1-ℓ2 minimization. We show that the proposed neural network is feasible and efficient for compressed sensing via simulation examples.},
author = {Liu, Yongwei and Hu, Jianfeng},
doi = {10.1016/j.neucom.2015.08.055},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Liu, Hu - 2016 - A neural network for ℓ1-ℓ2 minimization based on scaled gradient projection Application to compressed sensing.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Compressed sensing,Neural network,Scaled gradient projection,ℓ1-ℓ2 minimization},
pages = {988--993},
publisher = {Elsevier},
title = {{A neural network for ℓ1-ℓ2 minimization based on scaled gradient projection: Application to compressed sensing}},
url = {http://dx.doi.org/10.1016/j.neucom.2015.08.055},
volume = {173},
year = {2016}
}
@article{Karjol2018,
abstract = {In this work, we present a variant of multiple deep neural network (DNN) based speech enhancement method. We directly estimate clean speech spectrum as a weighted average of outputs from multiple DNNs. The weights are provided by a gating network. The multiple DNNs and the gating network are trained jointly. The objective function is set as the mean square logarithmic error between the target clean spectrum and the estimated spectrum. We conduct experiments using two and four DNNs using the TIMIT corpus with nine noise types (four seen noises and five unseen noises) taken from the AURORA database at four different signal-to-noise ratios (SNRs). We also compare the proposed method with a single DNN based speech enhancement scheme and existing multiple DNN schemes using segmental SNR, perceptual evaluation of speech quality (PESQ) and short-term objective intelligibility (STOI) as the evaluation metrics. These comparisons show the superiority of proposed method over baseline schemes in both seen and unseen noises. Specifically, we observe an absolute improvement of 0.07 and 0.04 in PESQ measure compared to single DNN when averaged over all noises and SNRs for seen and unseen noise cases respectively.},
author = {Karjol, Pavan and Kumar, M. Ajay and Ghosh, Prasanta Kumar},
doi = {10.1109/ICASSP.2018.8462649},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Karjol, Kumar, Ghosh - 2018 - Speech enhancement using multiple deep neural networks.pdf:pdf},
isbn = {9781538646588},
issn = {15206149},
journal = {ICASSP, IEEE Int. Conf. Acoust. Speech Signal Process. - Proc.},
keywords = {Deep neural networks,Gating network,Speech enhancement},
pages = {5049--5052},
publisher = {IEEE},
title = {{Speech enhancement using multiple deep neural networks}},
volume = {2018-April},
year = {2018}
}
@article{Chen2018,
abstract = {This paper presents a solution for simultaneous image encryption and compression. The primary introduced techniques are compressed sensing (CS) using structurally random matrix (SRM), and permutation-diffusion type image encryption. The encryption performance originates from both the techniques, whereas the compression effect is achieved by CS. Three-dimensional (3-D) cat map is employed for key stream generation. The simultaneously produced three state variables of 3-D cat map are respectively used for the SRM generation, image permutation and diffusion. Numerical simulations and security analyses have been carried out, and the results demonstrate the effectiveness and security performance of the proposed system.},
author = {Chen, Junxin and Zhang, Yu and Qi, Lin and Fu, Chong and Xu, Lisheng},
doi = {10.1016/j.optlastec.2017.09.008},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Chen et al. - 2018 - Exploiting chaos-based compressed sensing and cryptographic algorithm for image encryption and compression.pdf:pdf},
issn = {00303992},
journal = {Opt. Laser Technol.},
keywords = {3-D cat map,Compressed sensing,Image encryption,Structurally random matrix},
pages = {238--248},
publisher = {Elsevier Ltd},
title = {{Exploiting chaos-based compressed sensing and cryptographic algorithm for image encryption and compression}},
url = {https://doi.org/10.1016/j.optlastec.2017.09.008},
volume = {99},
year = {2018}
}
@article{Iliadis2018,
abstract = {In this work we present a deep learning framework for video compressive sensing. The proposed formulation enables recovery of video frames in a few seconds at significantly improved reconstruction quality compared to previous approaches. Our investigation starts by learning a linear mapping between video sequences and corresponding measured frames which turns out to provide promising results. We then extend the linear formulation to deep fully-connected networks and explore the performance gains using deeper architectures. Our analysis is always driven by the applicability of the proposed framework on existing compressive video architectures. Extensive simulations on several video sequences document the superiority of our approach both quantitatively and qualitatively. Finally, our analysis offers insights into understanding how dataset sizes and number of layers affect reconstruction performance while raising a few points for future investigation.},
author = {Iliadis, Michael and Spinoulas, Leonidas and Katsaggelos, Aggelos K.},
doi = {10.1016/j.dsp.2017.09.010},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Iliadis, Spinoulas, Katsaggelos - 2018 - Deep fully-connected networks for video compressive sensing.pdf:pdf},
issn = {10512004},
journal = {Digit. Signal Process. A Rev. J.},
keywords = {Deep neural networks,Fully-connected networks,Video compressive sensing},
pages = {9--18},
publisher = {Elsevier Inc.},
title = {{Deep fully-connected networks for video compressive sensing}},
url = {https://doi.org/10.1016/j.dsp.2017.09.010},
volume = {72},
year = {2018}
}
@article{Liu2014,
abstract = {An algorithm of principal component analysis in video compressed sensing is proposed in the paper. Aiming at the compressed sensing problems of video sequences, the inter-frame correlation among the images is analyzed and the transform coefficients with lower value are removed according to the energy concentration characteristics of principal component analysis. Therefore, the sparse realization of video signals in the form of principal component analysis is accomplished and the possibility of the transformation being used in compressed sensing algorithm is verified. Finally, simulation results show that, with the comparison of the traditional algorithm based on wavelet transform, the proposed algorithm can not only improve the reconstructed quality and the visual effects of the video sequence, but also save the sampling resources. Moreover, it is more suitable for stream transmission of multimedia. {\textcopyright} 2013 Elsevier GmbH.},
author = {Liu, Sheng and Gu, Mingming and Zhang, Qingchun and Li, Bing},
doi = {10.1016/j.ijleo.2013.07.120},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Liu et al. - 2014 - Principal component analysis algorithm in video compressed sensing.pdf:pdf},
issn = {00304026},
journal = {Optik (Stuttg).},
keywords = {Compressed sensing (CS),Eigenvector,Principal component analysis,Video compression},
number = {3},
pages = {1149--1153},
publisher = {Elsevier GmbH.},
title = {{Principal component analysis algorithm in video compressed sensing}},
url = {http://dx.doi.org/10.1016/j.ijleo.2013.07.120},
volume = {125},
year = {2014}
}
@article{Ramirez-Rubio2017,
abstract = {In this paper a new associative classification algorithm is presented. The proposed algorithm overcomes the limitations of the original Alpha-Beta associative memory, while maintaining the fundamental set recalling capacity. This algorithm has two phases. The first phase is based on an Alpha-Beta auto-associative memory, which works in the domain of real numbers, unlike the traditional Alpha-Beta associative memories. In the second phase, normalized difference between the results of first phase and every pattern of the fundamental set is calculated. In order to demonstrate the behaviour and accuracy of the algorithm, multiple well known datasets and classification algorithms have been used. Experimental results have shown that our proposal achieved the best performance in three of the eight pattern classification problems in the medical field, using Stratified 10 Fold cross-validation. Our proposal achieved the best classification accuracy averaged over the all datasets addressed in the present work. Experimental results and statistical significance tests, allow us to affirm that the proposed model is an efficient alternative to perform pattern classification tasks.},
author = {Ram{\'{i}}rez-Rubio, Rogelio and Aldape-P{\'{e}}rez, Mario and Y{\'{a}}{\~{n}}ez-M{\'{a}}rquez, Cornelio and L{\'{o}}pez-Y{\'{a}}{\~{n}}ez, Itzam{\'{a}} and Camacho-Nieto, Oscar},
doi = {10.1016/j.patrec.2017.02.013},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Ram{\'{i}}rez-Rubio et al. - 2017 - Pattern classification using smallest normalized difference associative memory.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognit. Lett.},
keywords = {Associative memories,Decision support systems,Hybrid algorithms,Pattern classification,Supervised machine learning},
pages = {104--112},
title = {{Pattern classification using smallest normalized difference associative memory}},
volume = {93},
year = {2017}
}
@article{Xie2016,
abstract = {In compressed sensing (CS), sparse or compressible signals can be reconstructed with fewer samples than the Nyquist–Shannon theorem requires. Over the past ten years, CS has developed into a relatively mature theory and this brand-new technique has been widely used in many fields such as image processing, wireless communication and medical imaging. In this paper, we propose a new model for signal compression and reconstruction based on semi-tensor product, called STP-CS, which is a generalization of traditional CS. Like traditional CS, we investigate some reconstruction conditions of STP-CS in terms of the spark, the coherence and the restricted isometry property (RIP). The experimental results show that STP-CS has the flexibility to choose a lower-dimensional sensing matrix for signal compression and reconstruction.},
author = {Xie, Dong and Peng, Haipeng and Li, Lixiang and Yang, Yixian},
doi = {10.1016/j.dsp.2016.07.003},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Xie et al. - 2016 - Semi-tensor compressed sensing.pdf:pdf},
issn = {10512004},
journal = {Digit. Signal Process. A Rev. J.},
keywords = {Coherence,Compressed sensing,Restricted isometry property,Semi-tensor product,Spark},
pages = {85--92},
publisher = {Elsevier Inc.},
title = {{Semi-tensor compressed sensing}},
url = {http://dx.doi.org/10.1016/j.dsp.2016.07.003},
volume = {58},
year = {2016}
}
@article{Mallat1994,
abstract = {Abstract not available},
author = {Mallat, Stephane G.},
doi = {10.1117/12.173207},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Mallat - 1994 - Adaptive time-frequency decompositions.pdf:pdf},
issn = {0091-3286},
journal = {Opt. Eng.},
number = {7},
pages = {2183},
title = {{Adaptive time-frequency decompositions}},
volume = {33},
year = {1994}
}
@article{Cui2020,
abstract = {Speech enhancement is a crucial and challenging task in many applications. A novel speech enhancement method based on the simple recurrent unit (SRU) is proposed in this paper. First, the log-power spectra of noisy and clean speeches are extracted. Then, the mapping relationship between noisy and clean speech spectra is learned by a multiple-layer stacked SRU network. Finally, the well-trained model is used to predict the corresponding clean speech spectra from the noisy speech spectra and the whole clean speech waveform can be recovered. Compared with the existing algorithms, DNN, LSTM and GRU, the proposed method achieves significant improvements at training speed and has capability to balance the performance and the training time. Experimental results demonstrate the validity and robustness of the proposed method.},
author = {Cui, Xingyue and Chen, Zhe and Yin, Fuliang},
doi = {10.1016/j.apacoust.2019.107019},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Cui, Chen, Yin - 2020 - Speech enhancement based on simple recurrent unit network.pdf:pdf},
issn = {1872910X},
journal = {Appl. Acoust.},
keywords = {Deep neural network,Power spectra,Simple recurrent unit,Speech enhancement},
pages = {107019},
publisher = {Elsevier Ltd},
title = {{Speech enhancement based on simple recurrent unit network}},
url = {https://doi.org/10.1016/j.apacoust.2019.107019},
volume = {157},
year = {2020}
}
@article{Xiao2018,
abstract = {In this paper, a truly sub-Nyquist sampling method for frequency estimation of sinusoidal signals in noise is presented. Basically speaking, sinusoidal signals are first sampled at multiple sampling rates lower than the Nyquist rate, and then a robust Chinese remainder theorem (CRT) is proposed to estimate the frequencies of interest from the aliased frequencies obtained by taking the discrete Fourier transform of the collected samples in each undersampled waveform. Compared with compressed sensing, this method can be easily implemented from the hardware point of view. This paper provides a thorough overview of the existing research results on the robust CRT during the last decade, and discusses some related open problems as well.},
author = {Xiao, Li and Xia, Xiang Gen},
doi = {10.1016/j.sigpro.2018.04.022},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Xiao, Xia - 2018 - Frequency determination from truly sub-Nyquist samplers based on robust Chinese remainder theorem.pdf:pdf},
issn = {01651684},
journal = {Signal Processing},
keywords = {Compressed sensing,Discrete fourier transform,Frequency estimation,Robust Chinese remainder theorem,Undersampling},
pages = {248--258},
publisher = {Elsevier B.V.},
title = {{Frequency determination from truly sub-Nyquist samplers based on robust Chinese remainder theorem}},
url = {https://doi.org/10.1016/j.sigpro.2018.04.022},
volume = {150},
year = {2018}
}
@article{Candes2006a,
abstract = {This paper considers the model problem of reconstructing an object from incomplete frequency samples. Consider a discrete-time signal f ∈ CN and a randomly chosen set of frequencies $\Omega$. It is possible to reconstruct f from the partial knowledge of its Fourier coefficients on the set $\Omega$? A typical result of this paper is as follows. Suppose that f is a superposition of T spikes f(t) = ∑$\tau$∈T f($\tau$)$\delta$(t-$\tau$) obeying T ≤ CM {\textperiodcentered} (log N)-1 {\textperiodcentered} $\Omega$ for some constant CM {\textgreater} 0. We do not know the locations of the spikes nor their amplitudes. Then with probability at least 1 - O(N-M, f can be reconstructed exactly as the solution to the ℓ1 minimization problem ming∑t=oN-1 g(t) , s.t.ĝ($\omega$) = f̂($\omega$) for all $\omega$ ∈ $\Omega$. In short, exact recovery may be obtained by solving a convex optimization problem. We give numerical values for CM which depend on the desired probability of success. Our result may be interpreted as a novel kind of nonlinear sampling theorem. In effect, it says that any signal made out of T spikes may be recovered by convex programming from almost every set of frequencies of size O( T {\textperiodcentered} log N). Moreover, this is nearly optimal in the sense that any method succeeding with probability 1 - O(N-M) would in general require a number of frequency samples at least proportional to T {\textperiodcentered} log N. The methodology extends to a variety of other situations and higher dimensions. For example, we show how one can reconstruct a piecewise constant (one- or two-dimensional) object from incomplete frequency samples - provided that the number of jumps (discontinuities) obeys the condition above - by minimizing other convex functionals such as the total variation of f. {\textcopyright} 2006 IEEE.},
archivePrefix = {arXiv},
arxivId = {math/0409186},
author = {Cand{\`{e}}s, Emmanuel J. and Romberg, Justin and Tao, Terence},
doi = {10.1109/TIT.2005.862083},
eprint = {0409186},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Cand{\`{e}}s, Romberg, Tao - 2006 - Robust uncertainty principles Exact signal reconstruction from highly incomplete frequency information.pdf:pdf},
issn = {00189448},
journal = {IEEE Trans. Inf. Theory},
keywords = {Convex optimization,Duality in optimization,Free probability,Image reconstruction,Linear programming,Random matrices,Sparsity,Total-variation minimization,Trigonometric expansions,Uncertainty principle},
number = {2},
pages = {489--509},
primaryClass = {math},
title = {{Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information}},
volume = {52},
year = {2006}
}
@article{Qi2018,
abstract = {As a greedy algorithm for compressed sensing reconstruction, generalized orthogonal matching pursuit (gOMP) has been proposed by Wang et al. [10] to recover sparse signals, which simply selects multiple indices without additional postprocessing operation. In this paper, we consider efficient method for the recovery of sparse signals that exhibit additional structure in the form of the non-zero coefficients occurring in clusters. A block version of gOMP is proposed, named block generalized orthogonal matching pursuit (BgOMP). Moreover, theoretical analysis based on restricted isometry property (RIP) for BgOMP is investigated. Simulation results show that BgOMP has considerable recovery performance comparable to state-of-the-art algorithms in terms of probability of exact reconstruction and running time.},
author = {Qi, Rui and Yang, Diwei and Zhang, Yujie and Li, Hongwei},
doi = {10.1016/j.sigpro.2018.06.023},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Qi et al. - 2018 - On recovery of block sparse signals via block generalized orthogonal matching pursuit.pdf:pdf},
issn = {01651684},
journal = {Signal Processing},
keywords = {Compressed sensing,Generalized orthogonal matching pursuit,Restricted isometry property,Sparse recovery},
pages = {34--46},
publisher = {Elsevier B.V.},
title = {{On recovery of block sparse signals via block generalized orthogonal matching pursuit}},
url = {https://doi.org/10.1016/j.sigpro.2018.06.023},
volume = {153},
year = {2018}
}
@article{Cho2014,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and {Van Merri{\"{e}}nboer}, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
doi = {10.3115/v1/d14-1179},
eprint = {1406.1078},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Cho et al. - 2014 - Learning phrase representations using RNN encoder-decoder for statistical machine translation.pdf:pdf},
isbn = {9781937284961},
journal = {EMNLP 2014 - 2014 Conf. Empir. Methods Nat. Lang. Process. Proc. Conf.},
pages = {1724--1734},
title = {{Learning phrase representations using RNN encoder-decoder for statistical machine translation}},
year = {2014}
}
@article{Wu2018,
abstract = {Linear encoding of sparse vectors is widely popular, but is commonly data-independent -- missing any possible extra (but a priori unknown) structure beyond sparsity. In this paper we present a new method to learn linear encoders that adapt to data, while still performing well with the widely used {\$}\backslashell{\_}1{\$} decoder. The convex {\$}\backslashell{\_}1{\$} decoder prevents gradient propagation as needed in standard gradient-based training. Our method is based on the insight that unrolling the convex decoder into {\$}T{\$} projected subgradient steps can address this issue. Our method can be seen as a data-driven way to learn a compressed sensing measurement matrix. We compare the empirical performance of 10 algorithms over 6 sparse datasets (3 synthetic and 3 real). Our experiments show that there is indeed additional structure beyond sparsity in the real datasets; our method is able to discover it and exploit it to create excellent reconstructions with fewer measurements (by a factor of 1.1-3x) compared to the previous state-of-the-art methods. We illustrate an application of our method in learning label embeddings for extreme multi-label classification, and empirically show that our method is able to match or outperform the precision scores of SLEEC, which is one of the state-of-the-art embedding-based approaches.},
archivePrefix = {arXiv},
arxivId = {1806.10175},
author = {Wu, Shanshan and Dimakis, Alexandros G. and Sanghavi, Sujay and Yu, Felix X. and Holtmann-Rice, Daniel and Storcheus, Dmitry and Rostamizadeh, Afshin and Kumar, Sanjiv},
eprint = {1806.10175},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Wu et al. - 2018 - Learning a Compressed Sensing Measurement Matrix via Gradient Unrolling.pdf:pdf},
title = {{Learning a Compressed Sensing Measurement Matrix via Gradient Unrolling}},
url = {http://arxiv.org/abs/1806.10175},
year = {2018}
}
@article{Kumar2016,
abstract = {In this paper we consider the problem of speech enhancement in real-world like conditions where multiple noises can simultaneously corrupt speech. Most of the current literature on speech enhancement focus primarily on presence of single noise in corrupted speech which is far from real-world environments. Specifically, we deal with improving speech quality in office environment where multiple stationary as well as non-stationary noises can be simultaneously present in speech. We propose several strategies based on Deep Neural Networks (DNN) for speech enhancement in these scenarios. We also investigate a DNN training strategy based on psychoacoustic models from speech coding for enhancement of noisy speech},
author = {Kumar, Anurag and Florencio, Dinei},
doi = {10.21437/Interspeech.2016-88},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Kumar, Florencio - 2016 - Speech enhancement in multiple-noise conditions using deep neural networks.pdf:pdf},
issn = {19909772},
journal = {Proc. Annu. Conf. Int. Speech Commun. Assoc. INTERSPEECH},
keywords = {Deep neural network,Multiple noise types,Psychoacoustic models,Speech enhancement},
pages = {3738--3742},
title = {{Speech enhancement in multiple-noise conditions using deep neural networks}},
volume = {08-12-Sept},
year = {2016}
}
@article{Donoho2003,
abstract = {Given a dictionary D = {\{}d(k){\}} of vectors d(k), we seek to represent a signal S as a linear combination S = summation operator(k) gamma(k)d(k), with scalar coefficients gamma(k). In particular, we aim for the sparsest representation possible. In general, this requires a combinatorial optimization process. Previous work considered the special case where D is an overcomplete system consisting of exactly two orthobases and has shown that, under a condition of mutual incoherence of the two bases, and assuming that S has a sufficiently sparse representation, this representation is unique and can be found by solving a convex optimization problem: specifically, minimizing the l(1) norm of the coefficients gamma. In this article, we obtain parallel results in a more general setting, where the dictionary D can arise from two or several bases, frames, or even less structured systems. We sketch three applications: separating linear features from planar ones in 3D data, noncooperative multiuser encoding, and identification of over-complete independent component models.},
author = {Donoho, David L. and Elad, Michael},
doi = {10.1073/pnas.0437847100},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Donoho, Elad - 2003 - Optimally sparse representation in general (nonorthogonal) dictionaries via L1 minimization.pdf:pdf},
issn = {00278424},
journal = {Proc. Natl. Acad. Sci. U. S. A.},
number = {5},
pages = {2197--2202},
title = {{Optimally sparse representation in general (nonorthogonal) dictionaries via L1 minimization}},
volume = {100},
year = {2003}
}
@article{Chen2014,
abstract = {Compressed sensing (CS) is a novel technology to acquire and reconstruct sparse signals below the Nyquist rate. It has great potential in image and video acquisition and processing. To effectively improve the sparsity of signal being measured and reconstructing efficiency, an encoding and decoding model of residual distributed compressive video sensing based on double side information (RDCVS-DSI) is proposed in this paper. Exploiting the characteristics of image itself in the frequency domain and the correlation between successive frames, the model regards the video frame in low quality as the first side information in the process of coding, and generates the second side information for the non-key frames using motion estimation and compensation technology at its decoding end. Performance analysis and simulation experiments show that the RDCVS-DSI model can rebuild the video sequence with high fidelity in the consumption of quite low complexity. About 1 {\{}{\~{}}{\}} 5 dB gain in the average peak signal-to-noise ratio of the reconstructed frames is observed, and the speed is close to the least complex DCVS, when compared with prior works on compressive video sensing.},
author = {Chen, Jian and Su, Kai Xiong and Wang, Wei Xing and Lan, Cheng Dong},
doi = {10.3724/SP.J.1004.2014.02316},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Chen et al. - 2014 - Residual distributed compressive video sensing based on double side information.pdf:pdf},
issn = {02544156},
journal = {Zidonghua Xuebao/Acta Autom. Sin.},
keywords = {Compressed sensing,Distributed compressive video sensing,Residual coding,Video coding},
number = {10},
pages = {2316--2323},
publisher = {The Chinese Association of Automation and The Institute of Automation, Chinese Academy of Sciences},
title = {{Residual distributed compressive video sensing based on double side information}},
url = {http://dx.doi.org/10.1016/S1874-1029(14)60363-3},
volume = {40},
year = {2014}
}
@article{Pascual2019,
author = {Pascual, Santiago and Serr{\`{a}}, Joan and Bonafonte, Antonio},
doi = {10.1016/j.specom.2019.09.001},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Pascual, Serr{\`{a}}, Bonafonte - 2019 - Time-domain Speech Enhancement Using Generative Adversarial Networks.pdf:pdf},
issn = {01676393},
journal = {Speech Commun.},
title = {{Time-domain Speech Enhancement Using Generative Adversarial Networks}},
year = {2019}
}
@article{Xia2015,
abstract = {This paper proposes a new recurrent neural network-based Kalman filter for speech enhancement, based on a noise-constrained least squares estimate. The parameters of speech signal modeled as autoregressive process are first estimated by using the proposed recurrent neural network and the speech signal is then recovered from Kalman filtering. The proposed recurrent neural network is globally asymptomatically stable to the noise-constrained estimate. Because the noise-constrained estimate has a robust performance against non-Gaussian noise, the proposed recurrent neural network-based speech enhancement algorithm can minimize the estimation error of Kalman filter parameters in non-Gaussian noise. Furthermore, having a low-dimensional model feature, the proposed neural network-based speech enhancement algorithm has a much faster speed than two existing recurrent neural networks-based speech enhancement algorithms. Simulation results show that the proposed recurrent neural network-based speech enhancement algorithm can produce a good performance with fast computation and noise reduction.},
author = {Xia, Youshen and Wang, Jun},
doi = {10.1016/j.neunet.2015.03.008},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Xia, Wang - 2015 - Low-dimensional recurrent neural network-based Kalman filter for speech enhancement.pdf:pdf},
issn = {18792782},
journal = {Neural Networks},
keywords = {Noise-constrained estimation,Non-Gaussian noise,Recurrent neural network,Speech enhancement},
pages = {131--139},
publisher = {Elsevier Ltd},
title = {{Low-dimensional recurrent neural network-based Kalman filter for speech enhancement}},
url = {http://dx.doi.org/10.1016/j.neunet.2015.03.008},
volume = {67},
year = {2015}
}
@article{Gan2018,
abstract = {Compressed sensing is a revolutionary sampling framework at a sub-Nyquist rate, which relies potentially on sensing matrix. In this paper, a large class of chaotic sensing matrices with low complexity, hardware-friendly implementation and desirable sampling efficiency is proposed based on topologically conjugate chaotic systems (TCcSs). Specifically, we first elaborate an independently and identically distributed chaotic stream, which is generated from a TCcS via our customized zone matching algorithm. Then, the chaotic stream is employed to construct the novel chaotic sensing matrix. Our framework encompasses various families of TCcSs for establishing sensing matrices, such as TCcSs of Tent chaotic system. Moreover, the mutual coherence of the proposed sensing matrices is investigated, and it shows that this kind of chaotic sensing matrices has similar sampling efficiency to that of the state-of-the-art sensing matrices. Experimental performances verify the correctness of the theoretical analysis and illustrate that the proposed matrices can provide comparable results against the existing ones.},
author = {Gan, Hongping and Xiao, Song and Zhao, Yimin},
doi = {10.1016/j.sigpro.2018.03.014},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Gan, Xiao, Zhao - 2018 - A large class of chaotic sensing matrices for compressed sensing.pdf:pdf},
issn = {01651684},
journal = {Signal Processing},
keywords = {Chaotic stream,Mutual coherence,Sensing matrix,Topologically conjugate function},
pages = {193--203},
publisher = {Elsevier B.V.},
title = {{A large class of chaotic sensing matrices for compressed sensing}},
url = {https://doi.org/10.1016/j.sigpro.2018.03.014},
volume = {149},
year = {2018}
}
@article{Song2018,
abstract = {Estimating error rates for firearm evidence identification is a fundamental challenge in forensic science. This paper describes the recently developed congruent matching cells (CMC) method for image comparisons, its application to firearm evidence identification, and its usage and initial tests for error rate estimation. The CMC method divides compared topography images into correlation cells. Four identification parameters are defined for quantifying both the topography similarity of the correlated cell pairs and the pattern congruency of the registered cell locations. A declared match requires a significant number of CMCs, i.e., cell pairs that meet all similarity and congruency requirements. Initial testing on breech face impressions of a set of 40 cartridge cases fired with consecutively manufactured pistol slides showed wide separation between the distributions of CMC numbers observed for known matching and known non-matching image pairs. Another test on 95 cartridge cases from a different set of slides manufactured by the same process also yielded widely separated distributions. The test results were used to develop two statistical models for the probability mass function of CMC correlation scores. The models were applied to develop a framework for estimating cumulative false positive and false negative error rates and individual error rates of declared matches and non-matches for this population of breech face impressions. The prospect for applying the models to large populations and realistic case work is also discussed. The CMC method can provide a statistical foundation for estimating error rates in firearm evidence identifications, thus emulating methods used for forensic identification of DNA evidence.},
author = {Song, John and Vorburger, Theodore V. and Chu, Wei and Yen, James and Soons, Johannes A. and Ott, Daniel B. and Zhang, Nien Fan},
doi = {10.1016/j.forsciint.2017.12.013},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Song et al. - 2018 - Estimating error rates for firearm evidence identifications in forensic science.pdf:pdf},
issn = {18726283},
journal = {Forensic Sci. Int.},
keywords = {Ballistics identification,CMC,Congruent matching cell,Error rate,Firearm,Forensics},
pages = {15--32},
publisher = {Elsevier Ireland Ltd},
title = {{Estimating error rates for firearm evidence identifications in forensic science}},
url = {http://dx.doi.org/10.1016/j.forsciint.2017.12.013},
volume = {284},
year = {2018}
}
@article{Huang2017,
abstract = {Frequency estimation of multiple sinusoids is significant in both theory and application. In some application scenarios, only sub-Nyquist samples are available to estimate the frequencies. A conventional approach is to sample the signals at several lower rates. In this paper, we address frequency estimation of the signals in the time domain through undersampled data. We analyze the impact of undersampling and demonstrate that three sub-Nyquist channels are generally enough to estimate the frequencies provided the undersampling ratios are pairwise coprime. We deduce the condition that leads to the failure of resolving frequency ambiguity when two coprime undersampling channels are utilized. When three-channel sub-Nyquist samples are used jointly, the frequencies can be determined uniquely and the correct frequencies are estimated. Numerical experiments verify the correctness of our analysis and conclusion.},
author = {Huang, Shan and Zhang, Haijian and Sun, Hong and Yu, Lei and Chen, Liwen},
doi = {10.1016/j.sigpro.2017.04.013},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Huang et al. - 2017 - Frequency estimation of multiple sinusoids with three sub-Nyquist channels.pdf:pdf},
issn = {01651684},
journal = {Signal Processing},
keywords = {Coprime sampling,Frequency estimation,Sub-Nyquist sampling},
pages = {96--101},
publisher = {Elsevier B.V.},
title = {{Frequency estimation of multiple sinusoids with three sub-Nyquist channels}},
url = {http://dx.doi.org/10.1016/j.sigpro.2017.04.013},
volume = {139},
year = {2017}
}
@article{Tsai2012,
abstract = {Digital forensics, which identifies the characteristics and origin of a digital device, has become a new field of research. If digital content will serve as evidence in court, similar to its non-digital counterparts, digital forensics can play a crucial role in identifying the source model or device. To achieve this goal, the relationship between an image and its camera model will be explored. Various image-related and hardware-related features are utilized in the proposed model by a support vector machine approach along with decision fusion techniques. Furthermore, the optimum feature subset to achieve the highest accuracy rate is also explored. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
author = {Tsai, Min Jen and Wang, Chen Sheng and Liu, Jung and Yin, Jin Sheng},
doi = {10.1016/j.csi.2011.10.006},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Tsai et al. - 2012 - Using decision fusion of feature selection in digital forensics for camera source model identification.pdf:pdf},
issn = {09205489},
journal = {Comput. Stand. Interfaces},
keywords = {Camera model identification,Decision fusion,Digital image forensics,Feature selection},
number = {3},
pages = {292--304},
publisher = {Elsevier B.V.},
title = {{Using decision fusion of feature selection in digital forensics for camera source model identification}},
url = {http://dx.doi.org/10.1016/j.csi.2011.10.006},
volume = {34},
year = {2012}
}
@article{Iliadis2020,
abstract = {In this paper, we propose an encoder-decoder neural network model referred to as DeepBinaryMask for video compressive sensing. In video compressive sensing one frame is acquired using a set of coded masks (sensing matrix) from which a number of video frames, equal to the number of coded masks, is reconstructed. The proposed framework is an end-to-end model where the sensing matrix is trained along with the video reconstruction. The encoder maps a video block to compressive measurements by learning the binary elements of the sensing matrix. The decoder is trained to map the measurements from a video patch back to a video block via several hidden layers of a Multi-Layer Perceptron network. The predicted video blocks are stacked together to recover the unknown video sequence. The reconstruction performance is found to improve when using the trained sensing mask from the network as compared to other mask designs such as random, across a wide variety of compressive sensing reconstruction algorithms. Finally, our analysis and discussion offers insights into understanding the characteristics of the trained mask designs that lead to the improved reconstruction quality.},
archivePrefix = {arXiv},
arxivId = {1607.03343},
author = {Iliadis, Michael and Spinoulas, Leonidas and Katsaggelos, Aggelos K.},
doi = {10.1016/j.dsp.2019.102591},
eprint = {1607.03343},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Iliadis, Spinoulas, Katsaggelos - 2020 - DeepBinaryMask Learning a binary mask for video compressive sensing.pdf:pdf},
issn = {10512004},
journal = {Digit. Signal Process. A Rev. J.},
keywords = {Binary mask,Compressive sensing,Deep learning,Mask optimization,Video reconstruction},
pages = {102591},
publisher = {Elsevier Inc.},
title = {{DeepBinaryMask: Learning a binary mask for video compressive sensing}},
url = {https://doi.org/10.1016/j.dsp.2019.102591},
volume = {96},
year = {2020}
}
@article{Donoho2006,
abstract = {Suppose x is an unknown vector in Ropf{\textless}sup{\textgreater}m{\textless}/sup{\textgreater} (a digital image or signal); we plan to measure n general linear functionals of x and then reconstruct. If x is known to be compressible by transform coding with a known transform, and we reconstruct via the nonlinear procedure defined here, the number of measurements n can be dramatically smaller than the size m. Thus, certain natural classes of images with m pixels need only n=O(m{\textless}sup{\textgreater}1/4{\textless}/sup{\textgreater}log{\textless}sup{\textgreater}5/2{\textless}/sup{\textgreater}(m)) nonadaptive nonpixel samples for faithful recovery, as opposed to the usual m pixel samples. More specifically, suppose x has a sparse representation in some orthonormal basis (e.g., wavelet, Fourier) or tight frame (e.g., curvelet, Gabor)-so the coefficients belong to an lscr{\textless}sub{\textgreater}p{\textless}/sub{\textgreater} ball for 0{\&}lt;ples1. The N most important coefficients in that expansion allow reconstruction with lscr{\textless}sub{\textgreater}2{\textless}/sub{\textgreater} error O(N{\textless}sup{\textgreater}1/2-1{\textless}/sup{\textgreater}p/). It is possible to design n=O(Nlog(m)) nonadaptive measurements allowing reconstruction with accuracy comparable to that attainable with direct knowledge of the N most important coefficients. Moreover, a good approximation to those N important coefficients is extracted from the n measurements by solving a linear program-Basis Pursuit in signal processing. The nonadaptive measurements have the character of "random" linear combinations of basis/frame elements. Our results use the notions of optimal recovery, of n-widths, and information-based complexity. We estimate the Gel'fand n-widths of lscr{\textless}sub{\textgreater}p{\textless}/sub{\textgreater} balls in high-dimensional Euclidean space in the case 0{\&}lt;ples1, and give a criterion identifying near- optimal subspaces for Gel'fand n-widths. We show that "most" subspaces are near-optimal, and show that convex optimization (Basis Pursuit) is a near-optimal way to extract information derived from these near-optimal subspaces},
author = {Donoho, David L.},
doi = {10.1109/TIT.2006.871582},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Donoho - 2006 - Compressed sensing.pdf:pdf},
issn = {00189448},
journal = {IEEE Trans. Inf. Theory},
keywords = {Adaptive sampling,Almost-spherical sections of Banach spaces,Basis Pursuit,Eigenvalues of random matrices,Gel'fand n-widths,Information-based complexity,Integrated sensing and processing,Minimum ℓ1-norm decomposition,Optimal recovery,Quotient-of-a-Subspace theorem,Sparse solution of linear equations},
number = {4},
pages = {1289--1306},
title = {{Compressed sensing}},
volume = {52},
year = {2006}
}
@article{Zoughi2020,
author = {Zoughi, Toktam and Homayounpour, Mohammad Mehdi and Deypir, Mahmood},
doi = {10.1016/j.eswa.2019.112840},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Zoughi, Homayounpour, Deypir - 2020 - Adaptive windows multiple deep residual networks for speech recognition.pdf:pdf},
issn = {09574174},
journal = {Expert Syst. Appl.},
keywords = {Speech recognition,Deep neural network,Adaptive wi},
pages = {112840},
publisher = {Elsevier Ltd},
title = {{Adaptive windows multiple deep residual networks for speech recognition}},
url = {https://doi.org/10.1016/j.eswa.2019.112840},
volume = {139},
year = {2020}
}
@article{Kucera2018,
abstract = {In this review, we explore determination of element contents in hair with neutron activation analysis (NAA) and particle induced X-ray emission (PIXE). Here we discuss factors that influence the accuracy and credibility of results, and conclusions drawn for forensic science. Hair structure, growth phase, deposition of trace elements in hair, sampling and washing procedures are important factors before analysis, whereas the availability of reference values or ranges of hair elemental composition for non-exposed populations, and toxicological considerations are vitally important for results interpretation. We present here selected applications of NAA and PIXE for testing hypothesis of criminal poisoning with toxic elements. In conclusion, we would like to emphasize the importance of future NAA and PIXE applications for accurate determination of toxic elements and for spatially resolved element contents in hair, respectively.},
author = {Ku{\v{c}}era, Jan and Kamen{\'{i}}k, Jan and Havr{\'{a}}nek, Vladim{\'{i}}r},
doi = {10.1016/j.forc.2017.12.002},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Ku{\v{c}}era, Kamen{\'{i}}k, Havr{\'{a}}nek - 2018 - Hair elemental analysis for forensic science using nuclear and related analytical methods.pdf:pdf},
issn = {24681709},
journal = {Forensic Chem.},
keywords = {Forensic analysis,Hair,Neutron activation analysis,Particle induced X-ray emission},
pages = {65--74},
title = {{Hair elemental analysis for forensic science using nuclear and related analytical methods}},
volume = {7},
year = {2018}
}
@article{Villena2018,
abstract = {Digital Forensics encompasses the recovery and investigation of data, images, and recordings found in digital devices in order to provide evidence in the court of law. This paper is devoted to the assessment of digital evidence which requires not only an understanding of the scientific technique that leads to improved quality of surveillance video recordings, but also of the legal principles behind it. Emphasis is given on the special treatment of image processing in terms of its handling and explanation that would be acceptable in a court of law. In this context, we propose a variational Bayesian approach to multiple-image super-resolution based on Super-Gaussian prior models that automatically enhances the quality of outdoor video recordings and estimates all the model parameters while preserving the authenticity, credibility and reliability of video data as digital evidence. The proposed methodology is validated both quantitatively and visually on synthetic videos generated from single images and real-life videos and applied to a real-life case of damages and stealing in a private property.},
author = {Villena, Salvador and Vega, Miguel and Mateos, Javier and Rosenberg, Duska and Murtagh, Fionn and Molina, Rafael and Katsaggelos, Aggelos K.},
doi = {10.1016/j.compind.2018.02.004},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Villena et al. - 2018 - Image super-resolution for outdoor digital forensics. Usability and legal aspects.pdf:pdf},
issn = {01663615},
journal = {Comput. Ind.},
keywords = {Legal aspects,Outdoor surveillance,Super-resolution,Usability},
pages = {34--47},
publisher = {Elsevier B.V.},
title = {{Image super-resolution for outdoor digital forensics. Usability and legal aspects}},
url = {https://doi.org/10.1016/j.compind.2018.02.004},
volume = {98},
year = {2018}
}
@article{Liu2013,
abstract = {Distributed compressed video sensing (DCVS) is a framework that integrates both compressed sensing and distributed video coding characteristics to achieve a low-complexity video coding. However, how to design an efficient reconstruction by leveraging more realistic signal models that go beyond simple sparsity is still an open challenge. In this paper, we propose a novel "undersampled" correlation noise model to describe compressively sampled video signals, and present a maximum-likelihood dictionary learning based reconstruction algorithm for DCVS, in which both the correlation and sparsity constraints are included in a new probabilistic model. Moreover, the signal recovery in our algorithm is performed during the process of dictionary learning, instead of being employed as an independent task. Experimental results show that our proposal compares favorably with other existing methods, with 0.1-3.5 dB improvements in the average PSNR, and a 2-9 dB gain for non-key frames when key frames are subsampled at an increased rate. {\textcopyright} 2013 Elsevier Inc. All rights reserved.},
author = {Liu, Haixiao and Song, Bin and Qin, Hao and Qiu, Zhiliang},
doi = {10.1016/j.jvcir.2013.08.007},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Liu et al. - 2013 - Dictionary learning based reconstruction for distributed compressed video sensing.pdf:pdf},
issn = {10473203},
journal = {J. Vis. Commun. Image Represent.},
keywords = {Compressed sensing,Dictionary learning,Distributed compressed video sensing,Distributed video coding,Energy minimization,Maximum likelihood,Sparse recovery,Undersampled correlation noise model},
number = {8},
pages = {1232--1242},
publisher = {Elsevier Inc.},
title = {{Dictionary learning based reconstruction for distributed compressed video sensing}},
url = {http://dx.doi.org/10.1016/j.jvcir.2013.08.007},
volume = {24},
year = {2013}
}
@article{Salimans2016,
abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3{\%}. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
archivePrefix = {arXiv},
arxivId = {arXiv:1606.03498v1},
author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
eprint = {arXiv:1606.03498v1},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Salimans et al. - 2016 - Improved techniques for training GANs.pdf:pdf},
issn = {10495258},
journal = {Adv. Neural Inf. Process. Syst.},
pages = {2234--2242},
title = {{Improved techniques for training GANs}},
year = {2016}
}
@article{Mohimani2009,
abstract = {In this paper, a fast algorithm for overcomplete sparse decomposition, called SL0, is proposed. The algorithm is essentially a method for obtaining sparse solutions of underdetermined systems of linear equations, and its applications include underdetermined Sparse Component Analysis (SCA), atomic decomposition on overcomplete dictionaries, compressed sensing, and decoding real field codes. Contrary to previous methods, which usually solve this problem by minimizing the L1 norm using Linear Programming (LP) techniques, our algorithm tries to directly minimize the L0 norm. It is experimentally shown that the proposed algorithm is about two to three orders of magnitude faster than the state-of-the-art interior-point LP solvers, while providing the same (or better) accuracy.},
archivePrefix = {arXiv},
arxivId = {arXiv:0809.2508v2},
author = {Mohimani, Hosein and Babaie-Zadeh, Massoud and Jutten, Christian},
doi = {10.1109/TSP.2008.2007606},
eprint = {arXiv:0809.2508v2},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Mohimani, Babaie-Zadeh, Jutten - 2009 - A fast approach for overcomplete sparse decomposition based on smoothed ℓ0 norm.pdf:pdf},
issn = {1053587X},
journal = {IEEE Trans. Signal Process.},
keywords = {Atomic decomposition,Blind source separation (BSS),Compressed sensing,Overcomplete signal representation,Sparse component analysis (SCA),Sparse decomposition,Sparse source separation},
number = {1},
pages = {289--301},
title = {{A fast approach for overcomplete sparse decomposition based on smoothed ℓ0 norm}},
volume = {57},
year = {2009}
}
@article{Pati1993,
abstract = {We describe a recursive algorithm to compute representations of functions with respect to nonorthogonal and possibly overcomplete dictionaries of elementary building blocks e.g. affine (wavelet) frames. We propose a modification to the matching pursuit algorithm of Mallat and Zhang (1992) that maintains full backward orthogonality of the residual (error) at every step and thereby leads to improved convergence. We refer to this modified algorithm as orthogonal matching pursuit (OMP). It is shown that all additional computation required for the OMP algorithm may be performed recursively},
author = {Pati, Y. C. and Rezaiifar, R. and Krishnaprasad, P. S.},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Pati, Rezaiifar, Krishnaprasad - 1993 - Orthogonal matching pursuit recursive function approximation with applications to wavelet decomp.pdf:pdf},
isbn = {0818641207},
issn = {10586393},
journal = {Conf. Rec. Asilomar Conf. Signals, Syst. Comput.},
pages = {40--44},
title = {{Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition}},
volume = {1},
year = {1993}
}
@article{Du2019,
abstract = {Recently, deep learning methods have made a significant improvement in compressive sensing image reconstruction task. In the existing methods, the scene is measured block by block due to the high computational complexity. This results in block-effect of the recovered images. In this paper, we propose a fully convolutional measurement network, where the scene is measured as a whole. The proposed method powerfully removes the block-effect since the structure information of scene images is preserved. To make the measure more flexible, the measurement and the recovery parts are jointly trained. From the experiments, it is shown that the results by the proposed method outperforms those by the existing methods in PSNR, SSIM, and visual effect.},
archivePrefix = {arXiv},
arxivId = {1712.01641},
author = {Du, Jiang and Xie, Xuemei and Wang, Chenye and Shi, Guangming and Xu, Xun and Wang, Yuxiang},
doi = {10.1016/j.neucom.2018.04.084},
eprint = {1712.01641},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Du et al. - 2019 - Fully convolutional measurement network for compressive sensing image reconstruction.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Block-effect,Compressive sensing,Convolutional neural network,Full image measurement,Fully convolutional measurement network},
pages = {105--112},
publisher = {Elsevier B.V.},
title = {{Fully convolutional measurement network for compressive sensing image reconstruction}},
url = {https://doi.org/10.1016/j.neucom.2018.04.084},
volume = {328},
year = {2019}
}
@article{Mathew2016,
author = {Mathew, Magnel Rose and Premanand, B.},
doi = {10.1016/j.protcy.2016.05.193},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Mathew, Premanand - 2016 - Sub-Nyquist Sampling of Acoustic Signals Based on Chaotic Compressed Sensing.pdf:pdf},
issn = {22120173},
journal = {Procedia Technol.},
keywords = {chaotic compressed sensing,compressed sensing,logistic map,random demodulator},
pages = {941--948},
publisher = {Elsevier B.V.},
title = {{Sub-Nyquist Sampling of Acoustic Signals Based on Chaotic Compressed Sensing}},
url = {http://dx.doi.org/10.1016/j.protcy.2016.05.193},
volume = {24},
year = {2016}
}
@article{Gan2018a,
abstract = {An effective sensing matrix can sample sparse or compressible signals without distortion provided that the matrix satisfies the low mutual coherence in the compressive sensing paradigm. In this work, we propose a novel structural chaotic sensing matrix (ScSM), which has the merits of both structured random sensing matrix and chaotic construction. The proposed ScSM first flips original signal, then fast and pseudo-randomly measures the flipped coefficients using a chaotic-based circulant operator, and at last, down-samples the resulting measurements to obtain the final samples. We elaborate the flipping permutation operator, chaotic-based circulant matrix, and down-sampling operator for the ScSM based on Chebyshev chaotic sequence. Moreover, the proposed ScSM is proven to yield low mutual coherence, which guarantees the desirable sampling efficiency. Experimental validations demonstrate the validity of the theory. Because of its well-designed structurally deterministic construction, the proposed ScSM has inherent superiority for storage, fast calculation, and hardware realization.},
author = {Gan, Hongping and Xiao, Song and Zhao, Yimin and Xue, Xiao},
doi = {10.1016/j.image.2018.06.004},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Gan et al. - 2018 - Construction of efficient and structural chaotic sensing matrix for compressive sensing.pdf:pdf},
issn = {09235965},
journal = {Signal Process. Image Commun.},
keywords = {Chebyshev chaotic sequence,Compressive sensing,Mutual coherence,Structural sensing matrix},
pages = {129--137},
publisher = {Elsevier Ltd},
title = {{Construction of efficient and structural chaotic sensing matrix for compressive sensing}},
url = {https://doi.org/10.1016/j.image.2018.06.004},
volume = {68},
year = {2018}
}
@article{Sharma2018,
abstract = {In this paper, we have explored the framework of compressed sensing (CS) and sparse representation (SR) to reduce the footprint of unit selection based speech synthesis (USS) system. In the CS based framework, footprint reduction is achieved by storing either CS measurements or signs of CS measurements, instead of storing the raw speech waveforms. For efficient reconstruction using CS measurements, the speech signal should have a sparse representation over a predefined basis/dictionary. Hence, in this work, we have also studied the effectiveness of sparse representation for compressing the speech waveform. The experimental results are demonstrated using an analytical dictionary (DCT matrix), and several learned dictionaries, derived using K-singular value decomposition (KSVD), method of optimal directions (MOD), greedy adaptive dictionary (GAD) and principal component analysis (PCA) algorithms. To further increase compression in SR based framework of footprint reduction, the significant coefficients of sparse vector are selected adaptively, based on the type of speech segment (e.g., voiced, unvoiced etc.). Experimental studies on two different Indian languages suggest that CS/SR based footprint reduction methods can be used as an alternative to existing compression methods employed in USS system.},
author = {Sharma, Pulkit and Abrol, Vinayak and Nivedita and Sao, Anil Kumar},
doi = {10.1016/j.csl.2018.05.003},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Sharma et al. - 2018 - Reducing footprint of unit selection based text-to-speech system using compressed sensing and sparse representati.pdf:pdf},
issn = {10958363},
journal = {Comput. Speech Lang.},
keywords = {Compressed sensing,Dictionary learning,Sparse representation,Speech synthesis},
pages = {191--208},
publisher = {Elsevier Ltd},
title = {{Reducing footprint of unit selection based text-to-speech system using compressed sensing and sparse representation}},
url = {https://doi.org/10.1016/j.csl.2018.05.003},
volume = {52},
year = {2018}
}
@article{Lu2015,
abstract = {In order to attain better reconstruction quality from compressive sensing (CS) of images, exploitation of the dependency or correlation patterns among the transform coefficients commonly has been employed. In this paper, we study a new image sensing technique, called compressive image sensing (CIS), with computational complexity O(m{\textless}sup{\textgreater}2{\textless}/sup{\textgreater}), where m denotes the length of a measurement vector y=$\phi$x, which is sampled from the signal x of length n via the sampling matrix $\phi$ with dimensionality m × n. CIS is basically a variation on compressive sampling. The contributions of CIS include: (i) reconstruction speed is extremely fast due to a closed-form solution being derived; (ii) certain reconstruction accuracy is preserved because significant components of x can be reconstructed with higher priority via an elaborately designed $\phi$; and (iii) in addition to conventional 1D sensing, we also study 2D separate sensing to enable simultaneous acquisition and compression of large-sized images.},
author = {Lu, Chun Shien and Chen, Hung Wei},
doi = {10.1016/j.ins.2015.07.017},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Lu, Chen - 2015 - Compressive image sensing for fast recovery from limited samples A variation on compressive sensing.pdf:pdf},
issn = {00200255},
journal = {Inf. Sci. (Ny).},
keywords = {Compressed sensing,Reconstruction,Sampling,Sparsity,Transform},
pages = {33--47},
publisher = {Elsevier Ltd.},
title = {{Compressive image sensing for fast recovery from limited samples: A variation on compressive sensing}},
url = {http://dx.doi.org/10.1016/j.ins.2015.07.017},
volume = {325},
year = {2015}
}
@article{Low2018,
abstract = {Compressive speech enhancement (CSE) has gained popularity in recent years as it bypasses the need for noise estimation. Parallel to that, modulation domain has been widely studied in speech applications as it offers a more compact representation and is closely associated with speech intelligibility enhancement. Motivated by the development in modulation domain and CSE, this paper seeks to explore the suitability of modulation domain based sparse reconstruction for use in CSE. The main idea is to study if the increased sparsity in the modulation domain would benefit sparse reconstruction in CSE. The findings reveal that modulation transformation is sparser and offers a stronger restricted isometry property (RIP) compared to the frequency transformation, which is essential for sparse recovery with a high probability. The results are then extended to show that the sparse reconstruction error in the modulation domain is upper bounded by the frequency domain. Experimental results in a CSE setting concur with the theoretical derivations, with modulation domain CSE outperforming the frequency domain CSE through different speech quality measures.},
author = {Low, Siow Yong},
doi = {10.1016/j.specom.2018.08.003},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Low - 2018 - Compressive speech enhancement in the modulation domain.pdf:pdf},
issn = {01676393},
journal = {Speech Commun.},
keywords = {Compressed sensing,Compressibility,Modulation domain,Modulation spectrum,Sparsity,Speech enhancement},
number = {August},
pages = {87--99},
publisher = {Elsevier},
title = {{Compressive speech enhancement in the modulation domain}},
url = {https://doi.org/10.1016/j.specom.2018.08.003},
volume = {102},
year = {2018}
}
@article{Abrol2015,
abstract = {We leverage the recent algorithmic advances in compressive sensing (CS), and propose a novel unsupervised voiced/nonvoiced (V/NV) detection method for compressively sensed speech signals. It attempts to exploit the fact that there is significant glottal activity during production of voiced speech while the same is not true for nonvoiced speech. This characteristic of the speech production mechanism is captured in the sparse feature vector derived using CS framework. Further, we propose an information theoretic metric, for V/NV classification, exploiting the sparsity of the extracted feature using a signal adaptive dictionary motivated by speech production mechanism. The final classification is done using an adaptive threshold selection scheme, which uses the temporal information of speech signals. While existing methods of feature extraction use speech samples directly, proposed method performs V/NV detection in compressively sensed speech signals (requiring very less memory), where existing time or frequency domain detection methods are not directly applicable. Hence, this method can be effective for various speech applications. Performance of the proposed method is studied on CMU-ARCTIC database, for eight types of additive noises, taken from the NOISEX database, at different signal-to-noise ratios (SNRs). The proposed method performs similar or better compared to the existing methods, especially at lower SNRs and this provide compelling evidence of the effectiveness of sparse feature vector for V/NV detection.},
author = {Abrol, Vinayak and Sharma, Pulkit and Sao, Anil Kumar},
doi = {10.1016/j.specom.2015.06.001},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Abrol, Sharma, Sao - 2015 - Voicednonvoiced detection in compressively sensed speech signals.pdf:pdf},
issn = {01676393},
journal = {Speech Commun.},
keywords = {Compressed sensing,Dictionary learning,Linear prediction,Sparse coding,Voiced/nonvoiced detection},
pages = {194--207},
publisher = {Elsevier B.V.},
title = {{Voiced/nonvoiced detection in compressively sensed speech signals}},
url = {http://dx.doi.org/10.1016/j.specom.2015.06.001},
volume = {72},
year = {2015}
}
@article{Chai2018,
author = {Chai, Xiuli and Zheng, Xiaoyu and Gan, Zhihua and Han, Daojun and Chen, Yiran},
doi = {10.1016/j.sigpro.2018.02.007},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Chai et al. - 2018 - An image encryption algorithm based on chaotic system and compressive sensing.pdf:pdf},
issn = {0165-1684},
journal = {Signal Processing},
keywords = {Compressive sensing (CS),Elementa,Image encryption},
number = {March},
pages = {124--144},
publisher = {Elsevier B.V.},
title = {{An image encryption algorithm based on chaotic system and compressive sensing}},
url = {https://doi.org/10.1016/j.sigpro.2018.02.007},
volume = {148},
year = {2018}
}
@article{Karampidis2018,
abstract = {Steganalysis and steganography are the two different sides of the same coin. Steganography tries to hide messages in plain sight while steganalysis tries to detect their existence or even more to retrieve the embedded data. Both steganography and steganalysis received a great deal of attention, especially from law enforcement. While cryptography in many countries is being outlawed or limited, cyber criminals or even terrorists are extensively using steganography to avoid being arrested with encrypted incriminating material in their possession. Therefore, understanding the ways that messages can be embedded in a digital medium –in most cases in digital images-, and knowledge of state of the art methods to detect hidden information, is essential in exposing criminal activity. Digital image steganography is growing in use and application. Many powerful and robust methods of steganography and steganalysis have been presented in the literature over the last few years. In this literature review, we will discuss and present various steganalysis techniques – from earlier ones to state of the art- used for detection of hidden data embedded in digital images using various steganography techniques.},
author = {Karampidis, Konstantinos and Kavallieratou, Ergina and Papadourakis, Giorgos},
doi = {10.1016/j.jisa.2018.04.005},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Karampidis, Kavallieratou, Papadourakis - 2018 - A review of image steganalysis techniques for digital forensics.pdf:pdf},
issn = {22142126},
journal = {J. Inf. Secur. Appl.},
keywords = {Deep learning,Digital forensics,Image steganalysis,Steganography,Universal steganalysis},
pages = {217--235},
publisher = {Elsevier Ltd},
title = {{A review of image steganalysis techniques for digital forensics}},
url = {https://doi.org/10.1016/j.jisa.2018.04.005},
volume = {40},
year = {2018}
}
@article{Wang2019,
abstract = {It is generally recognized that encrypting an original image into meaningless cipher image is an ideal method to protect image information. However, during transmission, the meaningless cipher image would draw attention and thus attract attacks. Recently, compressive sensing (CS) and carrier images have been utilized by Chai et al. to construct a novel image encryption scheme with visual security. However, in this scheme, some extra transmission is required for possible decryption besides the encrypted image. Moreover, the imperceptibility of the cipher image can be further improved and the recovered image quality would be severely degraded if unsuitable carrier images are selected. In this paper, we design a visually secure encryption scheme by using the parallel compressive sensing (PCS) counter mode and embedding technique. In order to achieve higher security level, Logistic-Tent system and 3-D Cat map are introduced to construct the measurement matrices and to disturb the order of the embedded information, respectively. Furthermore, experimental results demonstrate that the cipher image exhibits superior imperceptibility and the recovered image possesses more satisfactory quality, which is independent of the carrier image.},
author = {Wang, Hui and Xiao, Di and Li, Min and Xiang, Yanping and Li, Xinyan},
doi = {10.1016/j.sigpro.2018.10.001},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Wang et al. - 2019 - A visually secure image encryption scheme based on parallel compressive sensing.pdf:pdf},
issn = {01651684},
journal = {Signal Processing},
keywords = {Discrete wavelet transform,Image encryption,Integer discrete wavelet transform,Parallel compressive sensing,Visually secure cipher image},
pages = {218--232},
publisher = {Elsevier B.V.},
title = {{A visually secure image encryption scheme based on parallel compressive sensing}},
url = {https://doi.org/10.1016/j.sigpro.2018.10.001},
volume = {155},
year = {2019}
}
@article{Andrew2007,
abstract = {The l-bfgs limited-memory quasi-Newton method is the algorithm of choice for optimizing the parameters of large-scale log-linear models with L2 regularization, but it cannot be used for an L1-regularized loss due to its non-differentiability whenever some parameter is zero. Efficient algorithms have been proposed for this task, but they are impractical when the number of parameters is very large. We present an algorithm Orthant-Wise Limited-memory Quasi-Newton (owlqn), based on l-bfgs, that can efficiently optimize the L1-regularized log-likelihood of log-linear models with millions of parameters. In our experiments on a parse reranking task, our algorithm was several orders of magnitude faster than an alternative algorithm, and substantially faster than lbfgs on the analogous L2-regularized problem. We also present a proof that owl-qn is guaranteed to converge to a globally optimal parameter vector. 1.},
author = {Andrew, Galen and Gao, Jianfeng},
doi = {10.1145/1273496.1273501},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Andrew, Gao - 2007 - Scalable training of L1-regularized log-linear models.pdf:pdf},
journal = {ACM Int. Conf. Proceeding Ser.},
pages = {33--40},
title = {{Scalable training of L1-regularized log-linear models}},
volume = {227},
year = {2007}
}
@article{Hou2019,
abstract = {We present a new method for improving the performances of variational autoencoder (VAE). In addition to enforcing the deep feature consistent principle thus ensuring the VAE output and its corresponding input images to have similar deep features, we also implement a generative adversarial training mechanism to force the VAE to output realistic and natural images. We present experimental results to show that the VAE trained with our new method outperforms state of the art in generating face images with much clearer and more natural noses, eyes, teeth, hair textures as well as reasonable backgrounds. We also show that our method can learn powerful embeddings of input face images, which can be used to achieve facial attribute manipulation. Moreover we propose a multi-view feature extraction strategy to extract effective image representations, which can be used to achieve state of the art performance in facial attribute prediction.},
author = {Hou, Xianxu and Sun, Ke and Shen, Linlin and Qiu, Guoping},
doi = {10.1016/j.neucom.2019.03.013},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Hou et al. - 2019 - Improving variational autoencoder with deep feature consistent and generative adversarial training.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Facial attributes,GAN,Generative model,Image generation,VAE},
pages = {183--194},
publisher = {Elsevier B.V.},
title = {{Improving variational autoencoder with deep feature consistent and generative adversarial training}},
url = {https://doi.org/10.1016/j.neucom.2019.03.013},
volume = {341},
year = {2019}
}
@article{Donoho2006,
abstract = {Let x be a signal to be sparsely decomposed over a redundant dictionary A, i.e., a sparse coefficient vector s has to be found such that x = As. It is known that this problem is inherently unstable against noise, and to overcome this instability, Donoho, Elad and Temlyakov ["Stable recovery of sparse overcomplete representations in the presence of noise," IEEE Trans. Inf. Theory, vol. 52, no. 1, pp. 6-18, Jan. 2006] have proposed to use an "approximate" decomposition, that is, a decomposition satisfying ∥x - As∥2 ≤ $\delta$ rather than satisfying the exact equality x = As. Then, they have shown that if there is a decomposition with ∥s∥0 {\textless};; (1 + M-1)/2, where M denotes the coherence of the dictionary, this decomposition would be stable against noise. On the other hand, it is known that a sparse decomposition with ∥s∥0 {\textless};; (1/2)spark(A) is unique. In other words, although a decomposition with ∥s∥0 {\textless};; (1/2)spark(A) is unique, its stability against noise has been proved only for highly more restrictive decompositions satisfying ∥s∥0 {\textless};; (1 + M-1)/2, because usually (1 + M-1)/2 ≪ (1/2)spark(A). This limitation maybe had not been very important before, because ∥s∥0 {\textless};; (1 + M-1)/2 is also the bound which guaranties that the sparse decomposition can be found via minimizing the l1 norm, a classic approach for sparse decomposition. However, with the availability of new algorithms for sparse decomposition, namely SL0 and robust-SLO, it would be important to know whether or not unique sparse decompositions with (1 + M-1)/2 ≤ ∥s∥0 {\textless};; (1/2)spark(A) are stable. In this correspondence, we show that such decompositions are indeed stable. In other words, we extend the stability bou- - nd from ∥s∥0 {\textless};; (1 + M-1)/2 to the whole uniqueness range ∥s∥0 {\textless};; (1/2)spark(A). In summary, we show that all unique sparse decompositions are stably recoverable. Moreover, we see that sparser decompositions are "more stable".},
author = {Donoho, D.L. and Elad, M. and Temlyakov, V.N.},
doi = {10.1109/TIT.2005.860430},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Donoho, Elad, Temlyakov - 2006 - Stable recovery of sparse overcomplete representations in the presence of noise.pdf:pdf},
issn = {0018-9448},
journal = {IEEE Trans. Inf. Theory},
keywords = {Basis pursuit,Greedy algorithm,Matching pursuit,Mutual coherence,Overcomplete representation,Sparse representation,ℓ 1 -norm minimization},
month = {jan},
number = {1},
pages = {6--18},
title = {{Stable recovery of sparse overcomplete representations in the presence of noise}},
url = {http://ieeexplore.ieee.org/document/1564423/},
volume = {52},
year = {2006}
}
@article{Zhou2016,
abstract = {Most image encryption algorithms based on low-dimensional chaos systems bear security risks and suffer encryption data expansion when adopting nonlinear transformation directly. To overcome these weaknesses and reduce the possible transmission burden, an efficient image compression-encryption scheme based on hyper-chaotic system and 2D compressive sensing is proposed. The original image is measured by the measurement matrices in two directions to achieve compression and encryption simultaneously, and then the resulting image is re-encrypted by the cycle shift operation controlled by a hyper-chaotic system. Cycle shift operation can change the values of the pixels efficiently. The proposed cryptosystem decreases the volume of data to be transmitted and simplifies the keys distribution simultaneously as a nonlinear encryption system. Simulation results verify the validity and the reliability of the proposed algorithm with acceptable compression and security performance.},
author = {Zhou, Nanrun and Pan, Shumin and Cheng, Shan and Zhou, Zhihong},
doi = {10.1016/j.optlastec.2016.02.018},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Zhou et al. - 2016 - Image compression-encryption scheme based on hyper-chaotic system and 2D compressive sensing.pdf:pdf},
issn = {00303992},
journal = {Opt. Laser Technol.},
keywords = {Compressive sensing,Hyper-chaotic system,Image compression-encryption},
pages = {121--133},
publisher = {Elsevier},
title = {{Image compression-encryption scheme based on hyper-chaotic system and 2D compressive sensing}},
url = {http://dx.doi.org/10.1016/j.optlastec.2016.02.018},
volume = {82},
year = {2016}
}
@article{Li2018a,
abstract = {To obtain higher encryption efficiency and to realize the compression of quantum image, a quantum gray image encryption-compression scheme is designed based on quantum cosine transform and 5-dimensional hyperchaotic system. The original image is compressed by the quantum cosine transform and Zigzag scan coding, and then the compressed image is encrypted by the 5-dimensional hyperchaotic system. The proposed quantum image encryption-compression algorithm has larger key space and higher security, since the employed 5-dimensional hyperchaotic system has more complex dynamic behavior, better randomness and unpredictability than the low-dimensional hyper-chaotic system. Simulation and theoretical analyses show that the proposed quantum image encryption-compression scheme is superior to the corresponding classical image encryption scheme in term of efficiency and security.},
author = {Li, Xiao Zhen and Chen, Wei Wei and Wang, Yun Qian},
doi = {10.1007/s10773-018-3810-7},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Li, Chen, Wang - 2018 - Quantum Image Compression-Encryption Scheme Based on Quantum Discrete Cosine Transform.pdf:pdf},
issn = {15729575},
journal = {Int. J. Theor. Phys.},
keywords = {5D hyper-chaotic system,Quantum discrete cosine transform,Quantum image compression,Quantum image encryption,Zigzag scan coding},
number = {9},
pages = {2904--2919},
publisher = {International Journal of Theoretical Physics},
title = {{Quantum Image Compression-Encryption Scheme Based on Quantum Discrete Cosine Transform}},
volume = {57},
year = {2018}
}
@article{Sun2019,
author = {Sun, Yubao and Chen, Jiwei and Liu, Qingshan and Liu, Guangcan},
doi = {10.1016/j.patcog.2019.107051},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Sun et al. - 2019 - Learning Image Compressed Sensing with Sub-pixel Convolutional Generative Adversarial Network.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognit.},
keywords = {Compound loss,Compressed Sensing,Sub-pixel convolutional GAN},
pages = {107051},
publisher = {Elsevier Ltd},
title = {{Learning Image Compressed Sensing with Sub-pixel Convolutional Generative Adversarial Network}},
url = {https://doi.org/10.1016/j.patcog.2019.107051},
year = {2019}
}
@article{Genee2015,
abstract = {USER cloning is a fast and versatile method for engineering of plasmid DNA. We have developed a user friendly Web server tool that automates the design of optimal PCR primers for several distinct USER cloning-based applications. Our Web server, named AMUSER (Automated DNA Modifications with USER cloning), facilitates DNA assembly and introduction of virtually any type of site-directed mutagenesis by designing optimal PCR primers for the desired genetic changes. To demonstrate the utility, we designed primers for a simultaneous two-position site-directed mutagenesis of green fluorescent protein (GFP) to yellow fluorescent protein (YFP), which in a single step reaction resulted in a 94{\%} cloning efficiency. AMUSER also supports degenerate nucleotide primers, single insert combinatorial assembly, and flexible parameters for PCR amplification. AMUSER is freely available online at http://www.cbs.dtu.dk/services/AMUSER/.},
author = {Genee, Hans Jasper and Bonde, Mads Tvillinggaard and Bagger, Frederik Otzen and Jespersen, Jakob Berg and Sommer, Morten O.A. and Wernersson, Rasmus and Olsen, Lars R{\o}nn},
doi = {10.1021/sb500194z},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Genee et al. - 2015 - Software-supported user cloning strategies for site-directed mutagenesis and DNA assembly.pdf:pdf},
issn = {21615063},
journal = {ACS Synth. Biol.},
keywords = {DNA assembly,USER cloning,Web server,point mutation,primer design,site-directed mutagenesis},
number = {3},
pages = {342--349},
title = {{Software-supported user cloning strategies for site-directed mutagenesis and DNA assembly}},
volume = {4},
year = {2015}
}
@article{Yao2019,
abstract = {Most traditional algorithms for compressive sensing image reconstruction suffer from the intensive computation. Recently, deep learning-based reconstruction algorithms have been reported, which dramatically reduce the time complexity than iterative reconstruction algorithms. In this paper, we propose a novel Deep Residual Reconstruction Network (DR2-Net) to reconstruct the image from its Compressively Sensed (CS) measurement. The DR2-Net is proposed based on two observations: (1) linear mapping could reconstruct a high-quality preliminary image, and (2) residual learning could further improve the reconstruction quality. Accordingly, DR2-Net consists of two components, i.e., linear mapping network and residual network, respectively. Specifically, the fully-connected layer in neural network implements the linear mapping network. We then expand the linear mapping network to DR2-Net by adding several residual learning blocks to enhance the preliminary image. Extensive experiments demonstrate that the DR2-Net outperforms traditional iterative methods and recent deep learning-based methods by large margins at measurement rates 0.01, 0.04, 0.1, and 0.25, respectively. The code of DR2-Net has been released on: https://github.com/coldrainyht/caffe{\_}dr2.},
author = {Yao, Hantao and Dai, Feng and Zhang, Shiliang and Zhang, Yongdong and Tian, Qi and Xu, Changsheng},
doi = {10.1016/j.neucom.2019.05.006},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Yao et al. - 2019 - DR2-Net Deep Residual Reconstruction Network for image compressive sensing.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Convolutional neural networks,DR2-Net,Image compressive sensing},
pages = {483--493},
publisher = {Elsevier B.V.},
title = {{DR2-Net: Deep Residual Reconstruction Network for image compressive sensing}},
url = {https://doi.org/10.1016/j.neucom.2019.05.006},
volume = {359},
year = {2019}
}
@article{Candes2008a,
abstract = {It is now well understood that (1) it is possible to reconstruct sparse signals exactly from what appear to be highly incomplete sets of linear measurements and (2) that this can be done by constrained ℓ1 minimization. In this paper, we study a novel method for sparse signal recovery that in many situations outperforms ℓ1 minimization in the sense that substantially fewer measurements are needed for exact recovery. The algorithm consists of solving a sequence of weighted ℓ1- minimization problems where the weights used for the next iteration are computed from the value of the current solution. We present a series of experiments demonstrating the remarkable performance and broad applicability of this algorithm in the areas of sparse signal recovery, statistical estimation, error correction and image processing. Interestingly, superior gains are also achieved when our method is applied to recover signals with assumed near-sparsity in overcomplete representations-not by reweighting theℓ1 norm of the coefficient sequence as is common, but by reweighting the ℓ1 norm of the transformed object. An immediate consequence is the possibility of highly efficient data acquisition protocols by improving on a technique known as Compressive Sensing. {\textcopyright} 2008 Birkh{\"{a}}user Boston.},
archivePrefix = {arXiv},
arxivId = {0711.1612},
author = {Cand{\`{e}}s, Emmanuel J. and Wakin, Michael B. and Boyd, Stephen P.},
doi = {10.1007/s00041-008-9045-x},
eprint = {0711.1612},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Cand{\`{e}}s, Wakin, Boyd - 2008 - Enhancing sparsity by reweighted L1 minimization.pdf:pdf},
issn = {10695869},
journal = {J. Fourier Anal. Appl.},
keywords = {Compressive sensing,Dantzig selector,FOCUSS,Iterative reweighting,Sparsity,Underdetermined systems of linear equations,ℓ1-Minimization},
number = {5-6},
pages = {877--905},
title = {{Enhancing sparsity by reweighted L1 minimization}},
volume = {14},
year = {2008}
}
@article{Eslahi2016,
abstract = {Compressive sensing (CS) is a recently emerging technique and an extensively studied problem in signal and image processing, which enables joint sampling and compression into a unified approach. Recently, local smoothness and nonlocal self-similarity have both led to superior sparsity priors for CS image restoration. In this paper, first, a new sparsity measure called joint adaptive sparsity measure (JASM) is introduced. The proposed JASM enforces both local sparsity and nonlocal 3D sparsity in transform domain, concurrently, providing a powerful mechanism for characterizing the structured sparsities of natural image. More precisely, the local sparsity depicts the local smoothness redundancies exploited by an adaptively learned sparsifying basis, and the nonlocal 3D sparsity corresponds to the nonlocal self-similarity constraint achieved by a new proposed nonlocal statistical sparse modeling. Then, two novel techniques for high-fidelity CS image and video recovery via JASM are proposed. The proposed methods are formulated in the form of minimization functional under regularization-based framework which is solved via an efficient alternating minimization algorithm based on split Bregman framework. Comprehensive experimental results are reported to manifest the effectiveness of the proposed methods compared with the current state-of-the-art methods in CS image/video restoration.},
author = {Eslahi, Nasser and Aghagolzadeh, Ali and Andargoli, Seyed Mehdi Hosseini},
doi = {10.1016/j.neucom.2016.03.013},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Eslahi, Aghagolzadeh, Andargoli - 2016 - Imagevideo compressive sensing recovery using joint adaptive sparsity measure.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Compressive video sensing,Dictionary learning,Nonlocal self-similarity,Sparse recovery},
pages = {88--109},
publisher = {Elsevier},
title = {{Image/video compressive sensing recovery using joint adaptive sparsity measure}},
url = {http://dx.doi.org/10.1016/j.neucom.2016.03.013},
volume = {200},
year = {2016}
}
@article{Storn1997,
abstract = {A new heuristic approach for minimizing possibly nonlinear and non-differentiable con- tinuous space functions is presented. By means of an extensive testbed it is demonstrated that the new method converges faster and with more certainty than many other acclaimed global optimization methods. The new method requires few control variables, is robust, easy to use, and lends itselfvery well to parallel computation.},
author = {Storn, Rainer and Price, Kenneth V.},
doi = {https://doi.org/10.1023/A%3A1008202821328},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Storn, Price - 1997 - Differential Evolution – A Simple and Efficient Heuristic for global Optimization over Continuous Spaces.pdf:pdf},
issn = {1573-2916},
journal = {J. Glob. Optim.},
keywords = {Stochastic optimization,evolution strategy,genetic algorithm,global optimization,nonlinear optimization},
pages = {341},
title = {{Differential Evolution – A Simple and Efficient Heuristic for global Optimization over Continuous Spaces}},
url = {https://link.springer.com/article/10.1023/A:1008202821328},
volume = {11},
year = {1997}
}
@misc{Mallat1993,
abstract = {The authors introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions. These waveforms are chosen in order to best match the signal structures. Matching pursuits are general procedures to compute adaptive signal representations. With a dictionary of Gabor functions a matching pursuit defines an adaptive time-frequency transform. They derive a signal energy distribution in the time-frequency plane, which does not include interference terms, unlike Wigner and Cohen class distributions. A matching pursuit isolates the signal structures that are coherent with respect to a given dictionary. An application to pattern extraction from noisy signals is described. They compare a matching pursuit decomposition with a signal expansion over an optimized wavepacket orthonormal basis, selected with the algorithm of Coifman and Wickerhauser see (IEEE Trans. Informat. Theory, vol. 38, Mar. 1992)},
author = {Mallat, Stephane G. and Zhang, Zhifeng},
booktitle = {IEEE Trans. Signal Process.},
doi = {10.1109/78.258082},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Mallat, Zhang - 1993 - Matching Pursuits With Time-Frequency Dictionaries.pdf:pdf},
issn = {19410476},
number = {12},
pages = {3397--3415},
title = {{Matching Pursuits With Time-Frequency Dictionaries}},
volume = {41},
year = {1993}
}
@article{Chen2017,
abstract = {High reconstructed performance compressed video sensing (CVS) with low computational complexity and memory requirement is very challenging. In order to reconstruct the high quality video frames with low computational complexity, this paper proposes a tensor-based joint sparseness regularization CVS reconstruction model FrTVCST (fractional-order total variation combined with sparsifying transform), in which a high-order tensor fractional-order total variation (FrTV) regularization and a tensor discrete wavelet transform (DWT) L0 norm regularization are combined. Furthermore, an approach for choosing the regularization parameter that controls the influence of the two terms in this joint model is proposed. Afterwards, a tensor gradient projection algorithm extended from smoothed L0 (SL0) is deduced to solve this combined tensor FrTV and DWT joint regularization constrained minimization problem, using a smooth approximation of the L0 norm. Compared with several state-of-the-art CVS reconstruction algorithms, such as the Kronecker compressive sensing (KCS), generalized tensor compressive sensing (GTCS), N-way block orthogonal matching pursuit (N-BOMP), low-rank tensor compressive sensing (LRTCS), extensive experiments with commonly used video data sets show the competitive performance of the proposed algorithm with respect to the peak signal-to-noise ratio (PSNR) and subjective visual quality.},
author = {Chen, Gao and Li, Gang and Zhang, Jiashu},
doi = {10.1016/j.image.2017.03.021},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Chen, Li, Zhang - 2017 - Tensor compressed video sensing reconstruction by combination of fractional-order total variation and sparsifyi.pdf:pdf},
issn = {09235965},
journal = {Signal Process. Image Commun.},
keywords = {Compressed video sensing,Fractional-order total variation,Reconstruction,Smoothed L0,Tensor},
number = {March},
pages = {146--156},
publisher = {Elsevier B.V.},
title = {{Tensor compressed video sensing reconstruction by combination of fractional-order total variation and sparsifying transform}},
url = {http://dx.doi.org/10.1016/j.image.2017.03.021},
volume = {55},
year = {2017}
}
@article{Mazumdar2017,
abstract = {An associative memory is a framework of content-addressable memory that stores a collection of message vectors (or a dataset) over a neural network while enabling a neurally feasible mechanism to recover any message in the dataset from its noisy version. Designing an associative memory requires addressing two main tasks: 1) learning phase: given a dataset, learn a concise representation of the dataset in the form of a graphical model (or a neural network), 2) recall phase: given a noisy version of a message vector from the dataset, output the correct message vector via a neurally feasible algorithm over the network learnt during the learning phase. This paper studies the problem of designing a class of neural associative memories which learns a network representation for a large dataset that ensures correction against a large number of adversarial errors during the recall phase. Specifically, the associative memories designed in this paper can store dataset containing exp(n) n-length message vectors over a network with O(n) nodes and can tolerate $\sigma$(n/polylogn) adversarial errors. This paper carries out this memory design by mapping the learning phase and recall phase to the tasks of dictionary learning with a square dictionary and iterative error correction in an expander code, respectively.},
archivePrefix = {arXiv},
arxivId = {1611.09621},
author = {Mazumdar, Arya and Rawat, Ankit Singh},
eprint = {1611.09621},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Mazumdar, Rawat - 2017 - Associative memory using dictionary learning and expander decoding.pdf:pdf},
journal = {31st AAAI Conf. Artif. Intell. AAAI 2017},
pages = {267--273},
title = {{Associative memory using dictionary learning and expander decoding}},
year = {2017}
}
@article{Liu2019,
abstract = {This paper presents a deep associative neural network (DANN) based on unsupervised representation learning for associative memory. In brain, the knowledge is learnt by associating different types of sensory data, such as image and voice. The associative memory models which imitate such a learning process have been studied for decades but with simpler architectures they fail to deal with large scale complex data as compared with deep neural networks. Therefore, we define a deep architecture consisting of a perception layer and hierarchical propagation layers. To learn the network parameters, we define a probabilistic model for the whole network inspired from unsupervised representation learning models. The model is optimized by a modified contrastive divergence algorithm with a novel iterated sampling process. After training, given a new data or corrupted data, the correct label or corrupted part is associated by the network. The DANN is able to achieve many machine learning problems, including not only classification, but also depicting the data given a label and recovering corrupted images. Experiments on MNIST digits and CIFAR-10 datasets demonstrate the learning capability of the proposed DANN.},
author = {Liu, Jia and Gong, Maoguo and He, Haibo},
doi = {10.1016/j.neunet.2019.01.004},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Liu, Gong, He - 2019 - Deep associative neural network for associative memory based on unsupervised representation learning.pdf:pdf},
issn = {18792782},
journal = {Neural Networks},
keywords = {Associative memory,Deep neural network,Image recovery,Unsupervised representation learning},
pages = {41--53},
publisher = {Elsevier Ltd},
title = {{Deep associative neural network for associative memory based on unsupervised representation learning}},
url = {https://doi.org/10.1016/j.neunet.2019.01.004},
volume = {113},
year = {2019}
}
@article{Mazumdar2015,
abstract = {An associative memory is a structure learned from a dataset M of vectors (signals) in a way such that, given a noisy version of one of the vectors as input, the nearest valid vector from M (nearest neighbor) is provided as output, preferably via a fast iterative algorithm. Traditionally, binary (or q-ary) Hopfield neural networks are used to model the above structure. In this paper, for the first time, we propose a model of associative memory based on sparse recovery of signals. Our basic premise is simple. For a dataset, we learn a set of linear constraints that every vector in the dataset must satisfy. Provided these linear constraints possess some special properties, it is possible to cast the task of finding nearest neighbor as a sparse recovery problem. Assuming generic random models for the dataset, we show that it is possible to store super-polynomial or exponential number of n-length vectors in a neural network of size O(n). Furthermore, given a noisy version of one of the stored vectors corrupted in near-linear number of coordinates, the vector can be correctly recalled using a neurally feasible algorithm.},
author = {Mazumdar, Arya and Rawat, Ankit Singh},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Mazumdar, Rawat - 2015 - Associative memory via a sparse recovery model.pdf:pdf},
issn = {10495258},
journal = {Adv. Neural Inf. Process. Syst.},
pages = {2701--2709},
title = {{Associative memory via a sparse recovery model}},
year = {2015}
}
@article{Lima2020,
abstract = {{\textless}p{\textgreater}L-Asparaginase (L-ASNase) is a key component in the treatment of acute lymphoblastic leukemia (ALL), but several clinical disadvantages, such as immunogenicity and rapid clearance, are still present. We evaluated the possibility to synthesize a new L-ASNase from {\textless}em{\textgreater}Escherichia coli{\textless}/em{\textgreater} with human-like glycosylation and study the glycosylation effect on the biochemical properties of the enzyme. Six L-ASNase mutants were also created in which L-ASNase glycosylation sites were removed through site-directed mutagenesis. The WT L-ASNase was successfully expressed, secreted and glycosylated by an engineered {\textless}em{\textgreater}P. pastoris{\textless}/em{\textgreater} strain and presented predominantly Man{\textless}sub{\textgreater}5{\textless}/sub{\textgreater}GlcNAc{\textless}sub{\textgreater}2{\textless}/sub{\textgreater} glycans on its structure, which were then able to decrease L-ASNase immunogenicity {\textless}em{\textgreater}in vitro{\textless}/em{\textgreater}. The purified glycosylated L-ASNase has shown a 30-fold decrease in specific enzymatic activity compared to the non-glycosylated proteoform, but a triple mutant L-ASNase (3 M) was able to restore L-ASNase biological activity to significant levels. 3 M accumulated in the yeast periplasmic space and there presented a 28-fold increase in enzymatic activity when compared to the fully glycosylated proteoform. Both WT and 3 M L-ASNases presented increased stability in human serum compared to non-glycosylated L-ASNase. This study demonstrates the important effects of glycosylation on L-ASNase properties and opens up new possibilities to use glycosylated L-ASNases for the treatment of ALL.{\textless}/p{\textgreater}},
author = {Lima, Guilherme Meira and Effer, Brian and Biasoto, Henrique Pellin and Feijoli, Veronica and Pessoa, Adalberto and Palmisano, Giuseppe and Monteiro, Gisele},
doi = {10.1016/j.bej.2020.107516},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Lima et al. - 2020 - GLYCOSYLATION OF L-ASPARAGINASE FROM emE.coliem THROUGH YEAST EXPRESSION AND SITE-DIRECTED MUTAGENESIS.pdf:pdf},
journal = {Biochem. Eng. J.},
keywords = {site-directed mutagenesis},
number = {July 2019},
title = {{GLYCOSYLATION OF L-ASPARAGINASE FROM {\textless}em{\textgreater}E.coli{\textless}/em{\textgreater} THROUGH YEAST EXPRESSION AND SITE-DIRECTED MUTAGENESIS}},
url = {https://www.sciencedirect.com/science/article/pii/S1369703X20300310?dgcid=rss{\_}sd{\_}all{\&}utm{\_}source=researcher{\_}app{\&}utm{\_}medium=referral{\&}utm{\_}campaign=RESR{\_}MRKT{\_}Researcher{\_}inbound},
volume = {156},
year = {2020}
}
@article{Dabov2007,
abstract = {We propose a novel image denoising strategy based on an enhanced sparse representation in transform domain. The enhancement of the sparsity is achieved by grouping similar 2-D image fragments (e.g., blocks) into 3-D data arrays which we call "groups." Collaborative filtering is a special procedure developed to deal with these 3-D groups. We realize it using the three successive steps: 3-D transformation of a group, shrinkage of the transform spectrum, and inverse 3-D transformation. The result is a 3-D estimate that consists of the jointly filtered grouped image blocks. By attenuating the noise, the collaborative filtering reveals even the finest details shared by grouped blocks and, at the same time, it preserves the essential unique features of each individual block. The filtered blocks are then returned to their original positions. Because these blocks are overlapping, for each pixel, we obtain many different estimates which need to be combined. Aggregation is a particular averaging procedure which is exploited to take advantage of this redundancy. A significant improvement is obtained by a specially developed collaborative Wiener filtering. An algorithm based on this novel denoising strategy and its efficient implementation are presented in full detail; an extension to color-image denoising is also developed. The experimental results demonstrate that this computationally scalable algorithm achieves state-of-the-art denoising performance in terms of both peak signal-to-noise ratio and subjective visual quality.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Dabov, Kostadin and Foi, Alessandro and Katkovnik, Vladimir and Egiazarian, Karen},
doi = {10.1109/TIP.2007.901238},
eprint = {arXiv:1011.1669v3},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Dabov et al. - 2007 - Image denoising by sparse 3-D transform-domain collaborative filtering.pdf:pdf},
isbn = {1057-7149 VO - 16},
issn = {10577149},
journal = {IEEE Trans. Image Process.},
keywords = {3-D transform shrinkage,Adaptive grouping,Block matching,Image denoising,Sparsity},
number = {8},
pages = {2080--2095},
pmid = {17688213},
title = {{Image denoising by sparse 3-D transform-domain collaborative filtering}},
volume = {16},
year = {2007}
}
@article{Larkin2016,
abstract = {It is not obvious how to extend Shannon's original information entropy to higher dimensions, and many different approaches have been tried. We replace the English text symbol sequence originally used to illustrate the theory by a discrete, bandlimited signal. Using Shannon's later theory of sampling we derive a new and symmetric version of the second order entropy in 1D. The new theory then naturally extends to 2D and higher dimensions, where by naturally we mean simple, symmetric, isotropic and parsimonious. Simplicity arises from the direct application of Shannon's joint entropy equalities and inequalities to the gradient (del) vector field image embodying the second order relations of the scalar image. Parsimony is guaranteed by halving of the vector data rate using Papoulis' generalized sampling expansion. The new 2D entropy measure, which we dub delentropy, is underpinned by a computable probability density function we call deldensity. The deldensity captures the underlying spatial image structure and pixel co-occurrence. It achieves this because each scalar image pixel value is nonlocally related to the entire gradient vector field. Both deldensity and delentropy are highly tractable and yield many interesting connections and useful inequalities. The new measure explicitly defines a realizable encoding algorithm and a corresponding reconstruction. Initial tests show that delentropy compares favourably with the conventional intensity-based histogram entropy and the compressed data rates of lossless image encoders (GIF, PNG, WEBP, JP2K-LS and JPG-LS) for a selection of images. The symmetric approach may have applications to higher dimensions and problems concerning image complexity measures.},
archivePrefix = {arXiv},
arxivId = {1609.01117},
author = {Larkin, Kieran G.},
eprint = {1609.01117},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Larkin - 2016 - Reflections on Shannon Information In search of a natural information-entropy for images.pdf:pdf},
title = {{Reflections on Shannon Information: In search of a natural information-entropy for images}},
url = {http://arxiv.org/abs/1609.01117},
year = {2016}
}
@article{Leung2014,
abstract = {This paper develops two neural network models, based on Lagrange programming neural networks (LPNNs), for recovering sparse signals in compressive sampling. The first model is for the standard recovery of sparse signals. The second one is for the recovery of sparse signals from noisy observations. Their properties, including the optimality of the solutions and the convergence behavior of the networks, are analyzed. We show that for the first case, the network converges to the global minimum of the objective function. For the second case, the convergence is locally stable. {\textcopyright} 2013 Elsevier B.V.},
author = {Leung, Chi Sing and Sum, John and Constantinides, A. G.},
doi = {10.1016/j.neucom.2013.09.028},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Leung, Sum, Constantinides - 2014 - Recurrent networks for compressive sampling.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Neural circuit,Stability},
pages = {298--305},
publisher = {Elsevier},
title = {{Recurrent networks for compressive sampling}},
url = {http://dx.doi.org/10.1016/j.neucom.2013.09.028},
volume = {129},
year = {2014}
}
@article{Lotfi2018,
abstract = {In this paper we present a new algorithm for compressive sensing that makes use of binary measurement matrices and achieves exact recovery of ultra sparse vectors, in a single pass and without any iterations. Due to its noniterative nature, our algorithm is hundreds of times faster than {\$}\backslashell{\_}1{\$}-norm minimization, and methods based on expander graphs, both of which require multiple iterations. Our algorithm can accommodate nearly sparse vectors, in which case it recovers index set of the largest components, and can also accommodate burst noise measurements. Compared to compressive sensing methods that are guaranteed to achieve exact recovery of all sparse vectors, our method requires fewer measurements However, methods that achieve statistical recovery, that is, recovery of almost all but not all sparse vectors, can require fewer measurements than our method.},
archivePrefix = {arXiv},
arxivId = {arXiv:1708.03608v2},
author = {Lotfi, Mahsa and Vidyasagar, Mathukumalli},
doi = {10.1109/TSP.2018.2841881},
eprint = {arXiv:1708.03608v2},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Lotfi, Vidyasagar - 2018 - A fast noniterative algorithm for compressive sensing using binary measurement matrices.pdf:pdf},
issn = {1053587X},
journal = {IEEE Trans. Signal Process.},
keywords = {Compressive sensing,basis pursuit,deterministic methods,expander graphs,restricted isometry property,ultra sparse vector recovery},
number = {15},
pages = {4079--4089},
title = {{A fast noniterative algorithm for compressive sensing using binary measurement matrices}},
volume = {66},
year = {2018}
}
@article{Al-Sharif2018,
abstract = {Increasingly, Cyber–physical Systems are expected to operate in different environments and interconnect with a diverse set of systems, equipment, and networks. This openness to heterogeneity, diversity, and complexity introduces a new level of vulnerabilities, which adds to the consistent need for security including the digital forensics capabilities. Digital investigators utilize the information on the attacker's computer to find clues that may help in proving a case. One aspect is the digital evidence that can be extracted from the main memory (RAM), which includes live information about running programs. A program's states, represented by variables' values, vary in their scope and duration. This paper explores RAM artifacts of Java programs. Because JVMs can run on various platforms, we compare the same program on three different implementations of JVM from forensic perspectives. Our investigation model assumes no information is provided by the underlying OS or JVM. Our results show that a program's states can still be extracted even after the garbage collector is explicitly invoked, the software is stopped, or the JVM is terminated. This research helps investigators identify the software used to launch the attack and understand its internal flows. Investigators can utilize this information to accuse the perpetrators and recover from attacks.},
author = {Al-Sharif, Ziad A. and Al-Saleh, Mohammed I. and Alawneh, Luay M. and Jararweh, Yaser I. and Gupta, Brij},
doi = {10.1016/j.future.2018.07.028},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Al-Sharif et al. - 2018 - Live forensics of software attacks on cyber–physical systems.pdf:pdf},
issn = {0167739X},
journal = {Futur. Gener. Comput. Syst.},
keywords = {Digital evidence,Digital forensics,Evidence collection process,Execution path,Execution state,Memory forensics,Program's execution behavior},
publisher = {Elsevier B.V.},
title = {{Live forensics of software attacks on cyber–physical systems}},
url = {https://doi.org/10.1016/j.future.2018.07.028},
year = {2018}
}
@article{Candes2008b,
abstract = {Due to the availability and increased usage of multimedia applications, features such as compression and security has gained more importance. Here, we propose a key generation algorithm and a double image encryption scheme with combined compression and encryption. The keys for encryption are generated using a novel modified convolution and chaotic mapping technique. First, the four least significant bits of the two images were truncated and then combined after permutation using the proposed logistic mapping. Also, cellular automata based diffusion is performed on the resultant image to strengthen the security further. Here, both confusion and diffusion seem to be integrated thus improvising the encryption scheme. The performance results and the test of randomness of the key and the algorithm were found to be successful. Since two images are compressed and encrypted simultaneously, it is useful in real - time scenarios.},
author = {Candes, Emmanuel J. and Wakin, Michael B.},
doi = {10.1109/MSP.2007.914731},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Candes, Wakin - 2008 - An introduction to compressive sampling A sensingsampling paradigm that goes against the common knowledge in data.pdf:pdf},
issn = {10535888},
journal = {IEEE Signal Process. Mag.},
keywords = {Cellular automata,Chaotic logistic mapping,Compression,Convolution,Diffusion,Encryption},
number = {2},
pages = {21--30},
title = {{An introduction to compressive sampling: A sensing/sampling paradigm that goes against the common knowledge in data acquisition}},
volume = {25},
year = {2008}
}
@article{Zhou2018,
abstract = {The one-bit compressive sampling (CS) framework aims at alleviating the quantization burden on analog-to-digital converters by quantizing each sample to one bit, i.e., capturing just the signs of samples. Motivated by one-bit CS theory, this paper addresses a new type of data acquisition system to recover spectrally sparse signals. This system is composed of a random demodulator and a one-bit quantizer. The former yields the signal compressed samples while the latter records the sign of each sample. With the observation sign data, the signal is eventually recovered by using the binary iterative hard thresholding algorithm. Through numerical experiments, we demonstrate that our scheme is high-efficient for spectrally sparse signal recovery in the situations of low signal-to-noise ratio, stringent bit budget and weak sparsity.},
author = {Zhou, Han Fei and Huang, Lei and Li, Jian},
doi = {10.1016/j.dsp.2018.04.014},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Zhou, Huang, Li - 2018 - Compressive sampling for spectrally sparse signal recovery via one-bit random demodulator.pdf:pdf},
issn = {10512004},
journal = {Digit. Signal Process. A Rev. J.},
keywords = {Binary iterative hard thresholding (BIHT),One-bit compressive sampling,Random demodulator (RD),Spectrally sparse signals},
pages = {1--7},
publisher = {Elsevier Inc.},
title = {{Compressive sampling for spectrally sparse signal recovery via one-bit random demodulator}},
url = {https://doi.org/10.1016/j.dsp.2018.04.014},
volume = {81},
year = {2018}
}
@article{Candes2006b,
abstract = {Suppose we are given a vector f in a class ℱ ⊂ ℝN, e.g., a class of digital signals or digital images. How many linear measurements do we need to make about f to be able to recover f to within precision $\epsilon$ in the Euclidean (ℓ2) metric? This paper shows that if the objects of interest are sparse in a fixed basis or compressible, then it is possible to reconstruct f to within very high accuracy from a small number of random measurements by solving a simple linear program. More precisely, suppose that the n th largest entry of the vector |f| (or of its coefficients in a fixed basis) obeys |f|(n) ≤ R {\textperiodcentered} n -1/p, where R {\textgreater} 0 and p {\textgreater} 0. Suppose that we take measurements yk = 〈 f, Xk〉, k = 1,..., K, where the X k. are N-dimensional Gaussian vectors with independent standard normal entries. Then for each f obeying the decay estimate above for some 0 {\textless} p {\textless} 1 and with overwhelming probability, our reconstruction f:, defined as the solution to the constraints yk = 〈 f , Xk〉 with minimal ℓ1 norm, obeys ||f - f'||ℓ2 ≤ Cp {\textperiodcentered} R {\textperiodcentered} (K/log N)-r, r = 1/p - 1/2. There is a sense in which this result is optimal; it is generally impossible to obtain a higher accuracy from any set of $\kappa$ measurements whatsoever. The methodology extends to various other random measurement ensembles; for example, we show that similar results hold if one observes a few randomly sampled Fourier coefficients of f. In fact, the results are quite general and require only two hypotheses on the measurement ensemble which are detailed. {\textcopyright} 2006 IEEE.},
archivePrefix = {arXiv},
arxivId = {math/0410542},
author = {Candes, Emmanuel J. and Tao, Terence},
doi = {10.1109/TIT.2006.885507},
eprint = {0410542},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Candes, Tao - 2006 - Near-optimal signal recovery from random projections Universal encoding strategies.pdf:pdf},
issn = {00189448},
journal = {IEEE Trans. Inf. Theory},
keywords = {Concentration of measure,Convex optimization,Duality in optimization,Linear programming,Random matrices,Random projections,Signal recovery,Singular values of random matrices,Sparsity,Trigonometric expansions,Uncertainty principle},
number = {12},
pages = {5406--5425},
primaryClass = {math},
title = {{Near-optimal signal recovery from random projections: Universal encoding strategies?}},
volume = {52},
year = {2006}
}
@article{Wei2019,
abstract = {Due to the rapid development of GANs, there has been significant progress in the field of human video motion transfer which has a wide range of applications in computer vision and graphics. However, existing works only support motion-controllable video synthesis while appearances of different video components are bound together and uncontrollable, which means one person can only appear with the same clothing and background. Besides, most of these works are person-specific and require to train an individual model for each person, which is inflexible and inefficient. Therefore, we propose appearance composing GAN: a general method enabling control over not only human motions but also video appearances for arbitrary human subjects within only one model. The key idea is to exert layout-level appearance control on different video components and fuse them to compose the desired full video scene. Specifically, we achieve such appearance control by providing our model with optimal appearance conditioning inputs obtained separately for each component, allowing controllable component appearance synthesis for different people by changing the input appearance conditions accordingly. In terms of synthesis, a two-stage GAN framework is proposed to sequentially generate the desired body semantic layouts and component appearances, both are consistent with the input human motions and appearance conditions. Coupled with our ACGAN loss and background modulation block, the proposed method can achieve general and appearance-controllable human video motion transfer. Moreover, we build a dataset containing a large number of dance videos for training and evaluation. Experimental results show that, when applied to motion transfer tasks involving a variety of human subjects, our proposed method achieves appearance-controllable synthesis with higher video quality than state-of-arts based on only one-time training.},
archivePrefix = {arXiv},
arxivId = {1911.10672},
author = {Wei, Dongxu and Shen, Haibin and Huang, Kejie},
eprint = {1911.10672},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Wei, Shen, Huang - 2019 - Appearance Composing GAN A General Method for Appearance-Controllable Human Video Motion Transfer.pdf:pdf},
title = {{Appearance Composing GAN: A General Method for Appearance-Controllable Human Video Motion Transfer}},
url = {http://arxiv.org/abs/1911.10672},
year = {2019}
}
@article{Donoho2006a,
abstract = {We consider linear equations y = $\Phi$x where y is a given vector in ℝ n and $\Phi$ is a given n × m matrix with n {\textless} m ≤ $\tau$ n, and we wish to solve for x $\epsilon$ ℝ m. We suppose that the columns of $\Phi$ are normalized to the unit ℓ 2-norm, and we place uniform measure on such $\Phi$. We prove the existence of $\rho$ = $\rho$($\tau$) {\textgreater} 0 so that for large n and for all $\Phi$'s except a negligible fraction, the following property holds: For every y having a representation y = $\Phi$x 0 by a coefficient vector x 0 ∈ ℝ m with fewer than p {\textperiodcentered} n nonzeros, the solution x 1 of the ℓ-minimization problem min ||x|| 1 subject to $\Phi$x = y is unique and equal to x 0. In contrast, heuristic attempts to sparsely solve such systems - greedy algorithms and thresholding - perform poorly in this challenging setting. The techniques include the use of random proportional embeddings and almost-spherical sections in Banach space theory, and deviation bounds for the eigenvalues of random Wishart matrices. {\textcopyright} 2006 Wiley Periodicals, Inc.},
author = {Donoho, David L.},
doi = {10.1002/cpa.20132},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Donoho - 2006 - For most large underdetermined systems of linear equations the minimal L1-norm solution is also the sparsest solution.pdf:pdf},
issn = {00103640},
journal = {Commun. Pure Appl. Math.},
number = {6},
pages = {797--829},
title = {{For most large underdetermined systems of linear equations the minimal L1-norm solution is also the sparsest solution}},
volume = {59},
year = {2006}
}
@article{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
archivePrefix = {arXiv},
arxivId = {1406.2661},
author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
eprint = {1406.2661},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:pdf},
pages = {1--9},
title = {{Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1406.2661},
year = {2014}
}
@article{Tu2017,
abstract = {Speech enhancement under noise condition has always been an intriguing research topic. In this paper, we propose a new Deep Neural Networks (DNNs) based architecture for speech enhancement. In contrast to standard feed forward network architecture, we add skip connections between network in-puts and outputs to indirectly force the DNNs to learn ideal ratio mask. We also show that the performance can be fur-ther improved by stacking multiple such network blocks. Ex-perimental results demonstrate that our proposed architecture can achieve considerably better performance than the existing method in terms of three commonly used objective measure-ments under two real noise conditions.},
author = {Tu, Ming and Zhang, Xianxian},
doi = {10.1109/ICASSP.2017.7953221},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Tu, Zhang - 2017 - Speech enhancement based on Deep Neural Networks with skip connections.pdf:pdf},
isbn = {9781509041176},
issn = {15206149},
journal = {ICASSP, IEEE Int. Conf. Acoust. Speech Signal Process. - Proc.},
keywords = {Speech enhancement,deep neural networks,noise reduction,skip connections},
pages = {5565--5569},
title = {{Speech enhancement based on Deep Neural Networks with skip connections}},
year = {2017}
}
@misc{Curfman2009,
author = {Curfman, Gregory D. and Morrissey, Stephen and Drazen, Jeffrey M.},
booktitle = {N. Engl. J. Med.},
doi = {10.1056/NEJMe0902377},
file = {:D$\backslash$:/Kenneth/Documents/VIP/Papers/Curfman, Morrissey, Drazen - 2009 - The medical device safety act of 2009.pdf:pdf},
issn = {15334406},
number = {15},
pages = {1550--1551},
title = {{The medical device safety act of 2009}},
volume = {360},
year = {2009}
}
